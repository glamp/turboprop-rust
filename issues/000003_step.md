# Step 3: File Reading and Content Chunking

## Objective
Implement file reading and intelligent chunking strategy suitable for embedding models.

## Tasks
1. Implement file content reading with proper encoding handling
2. Create chunking algorithm based on embedding model dimensions
3. Add chunk overlap strategy for better search results
4. Implement chunk metadata tracking (file, line numbers, chunk index)
5. Add support for different file types (text files only initially)
6. Create comprehensive tests with poker codebase files

## Technical Approach
- Use sentence-transformers/all-MiniLM-L6-v2 (384 dimensions) as default model
- Chunk size should be ~200-500 tokens (appropriate for 384-dim model)
- Implement sliding window with 50-token overlap
- Track line numbers and character positions for each chunk

## Dependencies to Add
- `encoding_rs` - Character encoding detection
- `unicode-segmentation` - Text segmentation

## Acceptance Criteria
- Reads text files with proper encoding detection
- Chunks content with appropriate size for embedding model
- Maintains chunk overlap for continuity
- Tracks precise source locations (file, line range) for each chunk
- Handles edge cases (empty files, very small files, binary files)
- Works correctly with all poker codebase files
- Chunk metadata enables accurate result presentation

## Files Created/Modified
- `src/chunking.rs` - Content chunking logic
- `src/content.rs` - File content processing
- `src/types.rs` - Add chunk-related data structures
- `tests/chunking_tests.rs` - Comprehensive chunking tests

## Test Cases
- Test chunking on poker TypeScript files
- Test with small files (< chunk size)
- Test with large files
- Test overlap functionality
- Test line number tracking accuracy