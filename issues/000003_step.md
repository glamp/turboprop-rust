# Step 3: File Reading and Content Chunking

## Objective
Implement file reading and intelligent chunking strategy suitable for embedding models.

## Tasks
1. Implement file content reading with proper encoding handling
2. Create chunking algorithm based on embedding model dimensions
3. Add chunk overlap strategy for better search results
4. Implement chunk metadata tracking (file, line numbers, chunk index)
5. Add support for different file types (text files only initially)
6. Create comprehensive tests with poker codebase files

## Technical Approach
- Use sentence-transformers/all-MiniLM-L6-v2 (384 dimensions) as default model
- Chunk size should be ~200-500 tokens (appropriate for 384-dim model)
- Implement sliding window with 50-token overlap
- Track line numbers and character positions for each chunk

## Dependencies to Add
- `encoding_rs` - Character encoding detection
- `unicode-segmentation` - Text segmentation

## Acceptance Criteria
- Reads text files with proper encoding detection
- Chunks content with appropriate size for embedding model
- Maintains chunk overlap for continuity
- Tracks precise source locations (file, line range) for each chunk
- Handles edge cases (empty files, very small files, binary files)
- Works correctly with all poker codebase files
- Chunk metadata enables accurate result presentation

## Files Created/Modified
- `src/chunking.rs` - Content chunking logic
- `src/content.rs` - File content processing
- `src/types.rs` - Add chunk-related data structures
- `tests/chunking_tests.rs` - Comprehensive chunking tests

## Test Cases
- Test chunking on poker TypeScript files
- Test with small files (< chunk size)
- Test with large files
- Test overlap functionality
- Test line number tracking accuracy

## Proposed Solution

Based on the existing codebase structure, I will implement the file reading and chunking functionality using the following approach:

### 1. Dependencies
- Add `encoding_rs` for character encoding detection
- Add `unicode-segmentation` for proper text boundary detection  
- Add `tokenizers` or use a simple token approximation for chunk sizing

### 2. Data Structures (types.rs)
- `ContentChunk` - represents a chunk with content, metadata, and source location
- `ChunkingConfig` - configuration for chunk size, overlap, and model parameters
- `SourceLocation` - tracks file path, line range, and character positions

### 3. File Content Processing (content.rs)
- `ContentProcessor` - handles file reading with encoding detection
- Support for text files with proper encoding handling
- Binary file detection and filtering
- Content validation and sanitization

### 4. Chunking Algorithm (chunking.rs)
- `ChunkingStrategy` - implements sliding window with overlap
- Token-based chunking targeting 200-500 tokens for 384-dim embeddings
- 50-token overlap between adjacent chunks
- Precise line number and character position tracking
- Metadata preservation through chunk splitting

### 5. Integration
- Update existing `FileDiscovery` to optionally read and chunk file contents
- Maintain existing file filtering and discovery patterns
- Add chunking as an optional step in the indexing pipeline

### 6. Testing Strategy
- Unit tests for each component with edge cases
- Integration tests using poker codebase files
- Property-based tests for chunk overlap and completeness
- Performance tests with large files

This approach builds on the existing file discovery system while adding the content processing capabilities needed for embedding-based search.