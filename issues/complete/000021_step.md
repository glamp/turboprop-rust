# Step 21: Implement GGUF Model Support for Nomic Embed Code

## Objective
Implement support for loading and using GGUF models, specifically the `nomic-embed-code.Q5_K_S.gguf` model for code embedding generation.

## Background
The nomic-embed-code model is a 7B parameter quantized model optimized for code search, available in GGUF format. This requires implementing a new backend using candle-core or similar framework to load and run GGUF models.

## Tasks
1. Implement GGUF model loader using candle framework
2. Create embedding backend for GGUF models
3. Add model download and caching for GGUF files
4. Implement embedding generation for GGUF models
5. Add error handling specific to GGUF model operations
6. Add tests for GGUF model functionality

## Implementation Details

### Research Findings
- nomic-embed-code is available at: https://huggingface.co/nomic-ai/nomic-embed-code-GGUF/blob/main/nomic-embed-code.Q5_K_S.gguf
- Model is based on Qwen2 architecture and trained for multiple programming languages
- GGUF format requires specialized loading, not supported by fastembed
- Candle framework provides GGUF support in Rust

### 1. GGUF Backend Implementation
Create `src/backends/gguf.rs`:
```rust
use anyhow::{Context, Result};
use candle_core::{Device, Tensor};
use candle_transformers::models::qwen2::QWenConfig;
use std::path::Path;

pub struct GGUFBackend {
    device: Device,
}

impl GGUFBackend {
    pub fn new() -> Result<Self> {
        let device = Device::Cpu; // Start with CPU, add GPU support later
        Ok(Self { device })
    }
    
    pub fn load_model(&self, model_path: &Path) -> Result<GGUFEmbeddingModel> {
        // Load GGUF model using candle
        // Implementation details will use candle's GGUF loader
        todo!("Implement GGUF model loading")
    }
}

pub struct GGUFEmbeddingModel {
    model: Box<dyn candle_nn::Module>,
    tokenizer: tokenizers::Tokenizer,
    config: QWenConfig,
}

impl GGUFEmbeddingModel {
    pub fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        let mut embeddings = Vec::new();
        
        for text in texts {
            // Tokenize text
            let tokens = self.tokenizer.encode(text, false)
                .context("Failed to tokenize text")?;
            
            // Convert to tensor and run inference
            let input_tensor = Tensor::new(
                tokens.get_ids(),
                &self.device
            )?;
            
            // Run model inference
            let output = self.model.forward(&input_tensor)?;
            
            // Extract embeddings (typically from last hidden state)
            let embedding = self.extract_embedding(output)?;
            embeddings.push(embedding);
        }
        
        Ok(embeddings)
    }
    
    fn extract_embedding(&self, output: Tensor) -> Result<Vec<f32>> {
        // Extract final embedding vector from model output
        // Implementation depends on model architecture
        todo!("Implement embedding extraction")
    }
}
```

### 2. Model Download and Caching
Extend `src/models.rs` ModelManager:
```rust
impl ModelManager {
    pub async fn download_gguf_model(&self, model_info: &ModelInfo) -> Result<PathBuf> {
        if let Some(url) = &model_info.download_url {
            let model_path = self.get_model_path(&model_info.name);
            let file_path = model_path.join("model.gguf");
            
            if !file_path.exists() {
                std::fs::create_dir_all(&model_path)?;
                
                info!("Downloading GGUF model: {}", model_info.name);
                let response = reqwest::get(url).await?;
                let mut file = tokio::fs::File::create(&file_path).await?;
                
                let mut stream = response.bytes_stream();
                while let Some(chunk) = stream.try_next().await? {
                    tokio::io::AsyncWriteExt::write_all(&mut file, &chunk).await?;
                }
                
                info!("Downloaded GGUF model to: {:?}", file_path);
            }
            
            Ok(file_path)
        } else {
            Err(anyhow::anyhow!("No download URL provided for GGUF model"))
        }
    }
}
```

### 3. Integration with Embedding System
Update `src/embeddings.rs` to support GGUF backend:
```rust
pub enum EmbeddingBackendType {
    FastEmbed(TextEmbedding),
    GGUF(GGUFEmbeddingModel),
}

pub struct EmbeddingGenerator {
    backend: EmbeddingBackendType,
    config: EmbeddingConfig,
}

impl EmbeddingGenerator {
    pub async fn new_with_model(model_info: &ModelInfo) -> Result<Self> {
        let backend = match model_info.backend {
            ModelBackend::FastEmbed => {
                // Existing fastembed initialization
                let model = TextEmbedding::try_new(Default::default())?;
                EmbeddingBackendType::FastEmbed(model)
            },
            ModelBackend::Candle => {
                // New GGUF initialization
                let gguf_backend = GGUFBackend::new()?;
                let model_manager = ModelManager::default();
                let model_path = model_manager.download_gguf_model(model_info).await?;
                let model = gguf_backend.load_model(&model_path)?;
                EmbeddingBackendType::GGUF(model)
            },
            _ => return Err(anyhow::anyhow!("Unsupported backend: {:?}", model_info.backend)),
        };
        
        Ok(Self {
            backend,
            config: EmbeddingConfig::default(),
        })
    }
    
    pub fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        match &self.backend {
            EmbeddingBackendType::FastEmbed(model) => {
                // Existing fastembed implementation
                let embeddings = model.embed(texts.to_vec(), None)?;
                Ok(embeddings)
            },
            EmbeddingBackendType::GGUF(model) => {
                // New GGUF implementation
                model.embed(texts)
            },
        }
    }
}
```

### 4. Error Handling
Add GGUF-specific errors to `src/error.rs`:
```rust
#[derive(Debug, thiserror::Error)]
pub enum GGUFError {
    #[error("Failed to load GGUF model: {0}")]
    ModelLoadError(String),
    
    #[error("GGUF model inference failed: {0}")]
    InferenceError(String),
    
    #[error("Failed to download GGUF model: {0}")]
    DownloadError(String),
    
    #[error("Invalid GGUF model format: {0}")]
    InvalidFormatError(String),
}
```

## Files to Create/Modify
- `src/backends/` - New directory for backend implementations
- `src/backends/mod.rs` - Backend module exports
- `src/backends/gguf.rs` - GGUF backend implementation
- `src/embeddings.rs` - Update to support multiple backends
- `src/models.rs` - Add GGUF model download functionality
- `src/error.rs` - Add GGUF-specific error types
- `src/lib.rs` - Export new backend modules

## Acceptance Criteria
- [ ] GGUF model can be downloaded and cached
- [ ] GGUF model can be loaded using candle framework
- [ ] Embedding generation works for code text input
- [ ] Error handling provides clear feedback for GGUF operations
- [ ] Memory usage is reasonable for model size
- [ ] Performance is acceptable for code embedding tasks
- [ ] Integration tests verify GGUF functionality

## Success Validation
```bash
# Test GGUF model download (may take time for large model)
cargo test test_gguf_model_download -- --ignored

# Test GGUF embedding generation
cargo test test_gguf_embedding_generation

# Integration test with actual code samples
cargo run --bin tp -- index --repo . --model nomic-embed-code.Q5_K_S.gguf --limit 10
```

## Notes
- GGUF model is ~2.5GB, so initial download will be slow
- Consider adding progress bars for model downloads
- CPU inference may be slow; GPU support can be added later
- Model may require specific tokenizer configuration
- Start with basic implementation, optimize performance in later steps

## Proposed Solution

After analyzing the existing codebase, I found that much of the infrastructure is already in place:

### Current Architecture Analysis
- ✅ `ModelType::GGUF` and `ModelBackend::Candle` already defined in `src/types.rs`
- ✅ `EmbeddingBackend` and `EmbeddingModel` traits already defined in `src/models.rs`  
- ✅ GGUF model already listed in `ModelManager::get_available_models()` with correct HuggingFace URL
- ✅ All necessary dependencies (`candle-core`, `candle-transformers`, `tokenizers`, etc.) already in `Cargo.toml`
- ✅ Comprehensive error handling infrastructure already exists

### Implementation Plan

#### Phase 1: Backend Infrastructure (Foundation)
1. **Create backend module structure**:
   - `src/backends/mod.rs` - Export backend implementations
   - `src/backends/gguf.rs` - GGUF backend using candle framework
   - Update `src/lib.rs` to export backends module

2. **Implement GGUF-specific errors** in `src/error.rs`:
   - `GGUFModelLoadError`
   - `GGUFInferenceError` 
   - `GGUFDownloadError`
   - `GGUFFormatError`

#### Phase 2: GGUF Backend Implementation  
1. **`GGUFBackend` struct** implementing `EmbeddingBackend` trait:
   - Device management (CPU first, GPU later)
   - Model loading from local GGUF files using candle
   - Model validation and configuration

2. **`GGUFEmbeddingModel` struct** implementing `EmbeddingModel` trait:
   - Text tokenization using tokenizers crate
   - Model inference using candle-core
   - Embedding extraction from model outputs
   - Batch processing support

#### Phase 3: Model Management Extension
1. **Extend `ModelManager`** with GGUF functionality:
   - `download_gguf_model()` method with progress tracking
   - GGUF-specific model caching and validation
   - Model file integrity checks

2. **Add tokenizer management**:
   - Download and cache tokenizer files
   - Tokenizer configuration for nomic-embed-code model

#### Phase 4: Multi-Backend Integration
1. **Refactor `EmbeddingGenerator`** to support multiple backends:
   - Create `EmbeddingBackendType` enum wrapping FastEmbed and GGUF backends
   - Update constructor `new_with_model()` to select appropriate backend
   - Unified `embed()` method that dispatches to correct backend

2. **Model selection logic**:
   - Automatic backend selection based on `ModelInfo.backend`
   - Fallback mechanisms for unsupported models
   - Configuration validation

#### Phase 5: Testing & Validation
1. **Unit tests** (fast tier):
   - GGUF backend initialization (mocked)
   - Model loading error handling
   - Backend selection logic
   - Configuration validation

2. **Integration tests** (slow tier):
   - Actual GGUF model download (network)
   - Real model loading and inference  
   - End-to-end embedding generation
   - Performance benchmarks

### Key Technical Decisions

1. **CPU-First Approach**: Start with CPU inference for simplicity, add GPU support later
2. **Streaming Downloads**: Use `reqwest` with streaming for large GGUF model downloads
3. **Error Propagation**: Leverage existing `TurboPropError` infrastructure with new GGUF variants
4. **Backward Compatibility**: Existing FastEmbed functionality remains unchanged
5. **Model Caching**: Reuse existing model cache directory structure

### Implementation Order (TDD)
1. Write failing tests for GGUF backend traits
2. Implement minimal backend structure to make tests pass
3. Add model download functionality with tests
4. Implement embedding generation with tests  
5. Integrate with existing `EmbeddingGenerator`
6. Add comprehensive error handling and validation
7. Add performance optimizations and batching

This approach leverages the existing architecture while adding GGUF support in a clean, testable way that maintains backward compatibility.