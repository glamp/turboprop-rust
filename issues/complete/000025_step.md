# Step 25: Performance Optimization and Monitoring for New Models

## Objective
Optimize performance and add monitoring capabilities for the new embedding models, ensuring efficient resource usage and providing visibility into model performance characteristics.

## Background
The new models (GGUF and Qwen3) have different performance characteristics than the original sentence-transformer models:
- GGUF models are larger but quantized for efficiency
- Qwen3 models support instructions but may have different inference patterns
- Users need visibility into model performance to make informed choices

## Tasks
1. Implement performance monitoring and metrics collection
2. Optimize model loading and caching strategies
3. Add batch processing optimizations for new models
4. Implement memory usage monitoring and optimization
5. Add inference time tracking and reporting
6. Create performance comparison tools
7. Add resource usage warnings and recommendations

## Implementation Details

### 1. Performance Metrics Collection

Create `src/metrics.rs`:
```rust
use std::time::{Duration, Instant};
use std::sync::Arc;
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use serde::{Serialize, Deserialize};

/// Performance metrics for embedding operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbeddingMetrics {
    /// Model name
    pub model_name: String,
    /// Total number of texts embedded
    pub texts_embedded: usize,
    /// Total embedding time
    pub total_embedding_time: Duration,
    /// Average time per text
    pub avg_time_per_text: Duration,
    /// Peak memory usage during embedding
    pub peak_memory_usage: u64,
    /// Model loading time
    pub model_load_time: Option<Duration>,
    /// Cache hit rate for repeated texts
    pub cache_hit_rate: f32,
}

impl EmbeddingMetrics {
    pub fn new(model_name: String) -> Self {
        Self {
            model_name,
            texts_embedded: 0,
            total_embedding_time: Duration::ZERO,
            avg_time_per_text: Duration::ZERO,
            peak_memory_usage: 0,
            model_load_time: None,
            cache_hit_rate: 0.0,
        }
    }
    
    pub fn update_embedding_stats(&mut self, text_count: usize, duration: Duration) {
        self.texts_embedded += text_count;
        self.total_embedding_time += duration;
        self.avg_time_per_text = self.total_embedding_time / self.texts_embedded as u32;
    }
    
    pub fn set_model_load_time(&mut self, duration: Duration) {
        self.model_load_time = Some(duration);
    }
    
    pub fn update_memory_usage(&mut self, current_usage: u64) {
        self.peak_memory_usage = self.peak_memory_usage.max(current_usage);
    }
}

/// Thread-safe metrics collector
pub struct MetricsCollector {
    metrics: Arc<std::sync::Mutex<EmbeddingMetrics>>,
    start_time: Instant,
}

impl MetricsCollector {
    pub fn new(model_name: String) -> Self {
        Self {
            metrics: Arc::new(std::sync::Mutex::new(EmbeddingMetrics::new(model_name))),
            start_time: Instant::now(),
        }
    }
    
    pub fn record_embedding(&self, text_count: usize, duration: Duration) {
        if let Ok(mut metrics) = self.metrics.lock() {
            metrics.update_embedding_stats(text_count, duration);
        }
    }
    
    pub fn record_model_load_time(&self, duration: Duration) {
        if let Ok(mut metrics) = self.metrics.lock() {
            metrics.set_model_load_time(duration);
        }
    }
    
    pub fn get_metrics(&self) -> EmbeddingMetrics {
        self.metrics.lock().unwrap().clone()
    }
}

/// System resource monitor
pub struct ResourceMonitor {
    memory_samples: Vec<u64>,
    cpu_samples: Vec<f32>,
}

impl ResourceMonitor {
    pub fn new() -> Self {
        Self {
            memory_samples: Vec::new(),
            cpu_samples: Vec::new(),
        }
    }
    
    pub fn sample_resources(&mut self) {
        // Sample current memory usage
        if let Ok(memory) = self.get_memory_usage() {
            self.memory_samples.push(memory);
        }
        
        // Sample CPU usage (simplified)
        if let Ok(cpu) = self.get_cpu_usage() {
            self.cpu_samples.push(cpu);
        }
    }
    
    fn get_memory_usage(&self) -> Result<u64, Box<dyn std::error::Error>> {
        // Platform-specific memory usage collection
        #[cfg(target_os = "linux")]
        {
            let status = std::fs::read_to_string("/proc/self/status")?;
            for line in status.lines() {
                if line.starts_with("VmRSS:") {
                    let kb: u64 = line.split_whitespace()
                        .nth(1)
                        .ok_or("Invalid format")?
                        .parse()?;
                    return Ok(kb * 1024); // Convert to bytes
                }
            }
        }
        
        #[cfg(target_os = "macos")]
        {
            // Use system calls or external crates for macOS
            // Simplified implementation
            Ok(0)
        }
        
        #[cfg(target_os = "windows")]
        {
            // Use Windows API for memory information
            Ok(0)
        }
        
        Ok(0)
    }
    
    fn get_cpu_usage(&self) -> Result<f32, Box<dyn std::error::Error>> {
        // Simplified CPU usage (would need more sophisticated implementation)
        Ok(0.0)
    }
    
    pub fn get_peak_memory(&self) -> u64 {
        self.memory_samples.iter().copied().max().unwrap_or(0)
    }
    
    pub fn get_avg_memory(&self) -> u64 {
        if self.memory_samples.is_empty() {
            0
        } else {
            self.memory_samples.iter().sum::<u64>() / self.memory_samples.len() as u64
        }
    }
}
```

### 2. Optimized Model Loading

Update `src/embeddings.rs` with performance optimizations:
```rust
use crate::metrics::{MetricsCollector, ResourceMonitor};
use std::time::Instant;

pub struct OptimizedEmbeddingGenerator {
    backend: EmbeddingBackendType,
    config: EmbeddingConfig,
    metrics: MetricsCollector,
    resource_monitor: ResourceMonitor,
    embedding_cache: lru::LruCache<String, Vec<f32>>,
}

impl OptimizedEmbeddingGenerator {
    pub async fn new_with_model(model_info: &ModelInfo) -> Result<Self> {
        let start_time = Instant::now();
        let metrics = MetricsCollector::new(model_info.name.clone());
        
        // Load model with timing
        let backend = Self::load_backend_optimized(model_info).await?;
        
        let load_time = start_time.elapsed();
        metrics.record_model_load_time(load_time);
        
        tracing::info!(
            "Model '{}' loaded in {:.2}s",
            model_info.name,
            load_time.as_secs_f32()
        );
        
        Ok(Self {
            backend,
            config: EmbeddingConfig::default(),
            metrics,
            resource_monitor: ResourceMonitor::new(),
            embedding_cache: lru::LruCache::new(std::num::NonZeroUsize::new(1000).unwrap()),
        })
    }
    
    async fn load_backend_optimized(model_info: &ModelInfo) -> Result<EmbeddingBackendType> {
        match model_info.backend {
            ModelBackend::FastEmbed => {
                // Optimize fastembed loading
                let init_options = fastembed::InitOptions {
                    cache_dir: model_info.local_path.clone().unwrap_or_else(|| 
                        std::env::var("FASTEMBED_CACHE_PATH")
                            .unwrap_or_else(|_| ".turboprop/models".to_string())
                            .into()
                    ),
                    ..Default::default()
                };
                
                let model = fastembed::TextEmbedding::try_new(init_options)?;
                Ok(EmbeddingBackendType::FastEmbed(model))
            },
            
            ModelBackend::Candle => {
                // Optimize GGUF loading with memory mapping
                let gguf_backend = crate::backends::gguf::GGUFBackend::new_optimized()?;
                let model_manager = crate::models::ModelManager::default();
                let model_path = model_manager.download_gguf_model(model_info).await?;
                let model = gguf_backend.load_model_optimized(&model_path).await?;
                Ok(EmbeddingBackendType::GGUF(model))
            },
            
            ModelBackend::Custom => {
                // Optimize Hugging Face loading with caching
                let hf_backend = crate::backends::huggingface::HuggingFaceBackend::new_optimized().await?;
                let model = hf_backend.load_qwen3_model_optimized(&model_info.name).await?;
                Ok(EmbeddingBackendType::HuggingFace(model))
            },
        }
    }
    
    pub fn embed_with_monitoring(&mut self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        let start_time = Instant::now();
        self.resource_monitor.sample_resources();
        
        // Check cache for repeated texts
        let mut results = Vec::with_capacity(texts.len());
        let mut uncached_texts = Vec::new();
        let mut uncached_indices = Vec::new();
        
        for (i, text) in texts.iter().enumerate() {
            if let Some(cached_embedding) = self.embedding_cache.get(text) {
                results.push(Some(cached_embedding.clone()));
            } else {
                results.push(None);
                uncached_texts.push(text.clone());
                uncached_indices.push(i);
            }
        }
        
        // Process uncached texts in optimized batches
        if !uncached_texts.is_empty() {
            let batch_embeddings = self.embed_batch_optimized(&uncached_texts)?;
            
            // Update cache and results
            for (batch_idx, original_idx) in uncached_indices.iter().enumerate() {
                let embedding = batch_embeddings[batch_idx].clone();
                self.embedding_cache.put(uncached_texts[batch_idx].clone(), embedding.clone());
                results[*original_idx] = Some(embedding);
            }
        }
        
        // Flatten results
        let final_embeddings: Vec<Vec<f32>> = results.into_iter()
            .map(|opt| opt.unwrap())
            .collect();
        
        let duration = start_time.elapsed();
        self.metrics.record_embedding(texts.len(), duration);
        self.resource_monitor.sample_resources();
        
        tracing::debug!(
            "Embedded {} texts in {:.2}ms (avg: {:.2}ms/text)",
            texts.len(),
            duration.as_millis(),
            duration.as_millis() as f32 / texts.len() as f32
        );
        
        Ok(final_embeddings)
    }
    
    fn embed_batch_optimized(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        // Determine optimal batch size based on model type and available memory
        let optimal_batch_size = self.calculate_optimal_batch_size(texts.len());
        
        if texts.len() <= optimal_batch_size {
            // Process as single batch
            self.embed_single_batch(texts)
        } else {
            // Process in multiple optimized batches
            let mut all_embeddings = Vec::new();
            
            for chunk in texts.chunks(optimal_batch_size) {
                let batch_embeddings = self.embed_single_batch(chunk)?;
                all_embeddings.extend(batch_embeddings);
            }
            
            Ok(all_embeddings)
        }
    }
    
    fn embed_single_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        match &self.backend {
            EmbeddingBackendType::FastEmbed(model) => {
                let embeddings = model.embed(texts.to_vec(), None)?;
                Ok(embeddings)
            },
            EmbeddingBackendType::GGUF(model) => {
                model.embed_optimized(texts)
            },
            EmbeddingBackendType::HuggingFace(model) => {
                model.embed_optimized(texts)
            },
        }
    }
    
    fn calculate_optimal_batch_size(&self, text_count: usize) -> usize {
        // Calculate based on model type and available memory
        match &self.backend {
            EmbeddingBackendType::FastEmbed(_) => {
                // FastEmbed typically handles batching well
                std::cmp::min(text_count, 32)
            },
            EmbeddingBackendType::GGUF(_) => {
                // GGUF models may have memory constraints
                std::cmp::min(text_count, 8)
            },
            EmbeddingBackendType::HuggingFace(_) => {
                // Adjust based on model size
                std::cmp::min(text_count, 16)
            },
        }
    }
    
    pub fn get_performance_report(&self) -> PerformanceReport {
        let metrics = self.metrics.get_metrics();
        let peak_memory = self.resource_monitor.get_peak_memory();
        
        PerformanceReport {
            model_name: metrics.model_name,
            total_texts_processed: metrics.texts_embedded,
            total_processing_time: metrics.total_embedding_time,
            average_time_per_text: metrics.avg_time_per_text,
            model_load_time: metrics.model_load_time,
            peak_memory_usage: peak_memory,
            cache_efficiency: self.calculate_cache_efficiency(),
        }
    }
    
    fn calculate_cache_efficiency(&self) -> f32 {
        // Calculate cache hit rate
        let total_capacity = self.embedding_cache.cap().get() as f32;
        let current_size = self.embedding_cache.len() as f32;
        current_size / total_capacity
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PerformanceReport {
    pub model_name: String,
    pub total_texts_processed: usize,
    pub total_processing_time: Duration,
    pub average_time_per_text: Duration,
    pub model_load_time: Option<Duration>,
    pub peak_memory_usage: u64,
    pub cache_efficiency: f32,
}

impl PerformanceReport {
    pub fn print_summary(&self) {
        println!("Performance Report for {}", self.model_name);
        println!("=" * 50);
        println!("Texts processed: {}", self.total_texts_processed);
        println!("Total time: {:.2}s", self.total_processing_time.as_secs_f32());
        println!("Avg time/text: {:.2}ms", self.average_time_per_text.as_millis());
        
        if let Some(load_time) = self.model_load_time {
            println!("Model load time: {:.2}s", load_time.as_secs_f32());
        }
        
        println!("Peak memory: {}", format_bytes(self.peak_memory_usage));
        println!("Cache efficiency: {:.1}%", self.cache_efficiency * 100.0);
    }
}

fn format_bytes(bytes: u64) -> String {
    const UNITS: &[&str] = &["B", "KB", "MB", "GB"];
    let mut size = bytes as f64;
    let mut unit_index = 0;
    
    while size >= 1024.0 && unit_index < UNITS.len() - 1 {
        size /= 1024.0;
        unit_index += 1;
    }
    
    format!("{:.1} {}", size, UNITS[unit_index])
}
```

### 3. Performance Comparison Tools

Create `src/commands/benchmark.rs`:
```rust
use anyhow::Result;
use clap::Args;
use std::time::Instant;
use tabled::{Table, Tabled};

use crate::models::ModelManager;
use crate::embeddings::OptimizedEmbeddingGenerator;

#[derive(Args)]
pub struct BenchmarkArgs {
    /// Models to benchmark (default: all available)
    #[arg(long, value_delimiter = ',')]
    models: Option<Vec<String>>,
    
    /// Number of texts to process for benchmark
    #[arg(long, default_value = "100")]
    text_count: usize,
    
    /// Number of benchmark iterations
    #[arg(long, default_value = "3")]
    iterations: usize,
    
    /// Sample text file to use for benchmarking
    #[arg(long)]
    sample_file: Option<std::path::PathBuf>,
    
    /// Output format (table, json, csv)
    #[arg(long, default_value = "table")]
    format: String,
}

#[derive(Tabled)]
struct BenchmarkResult {
    model: String,
    texts_per_second: f32,
    avg_latency_ms: f32,
    model_load_time_s: f32,
    peak_memory_mb: f32,
    cache_efficiency: f32,
}

pub async fn run_benchmark(args: BenchmarkArgs) -> Result<()> {
    let available_models = ModelManager::get_available_models();
    let models_to_test = if let Some(model_names) = args.models {
        available_models.into_iter()
            .filter(|m| model_names.contains(&m.name))
            .collect()
    } else {
        available_models
    };
    
    if models_to_test.is_empty() {
        return Err(anyhow::anyhow!("No models available for benchmarking"));
    }
    
    // Generate or load test texts
    let test_texts = if let Some(sample_file) = args.sample_file {
        load_sample_texts(&sample_file, args.text_count)?
    } else {
        generate_test_texts(args.text_count)
    };
    
    println!("Running benchmark with {} texts across {} iterations...\n", 
             test_texts.len(), args.iterations);
    
    let mut results = Vec::new();
    
    for model_info in models_to_test {
        println!("Benchmarking model: {}", model_info.name);
        
        let mut iteration_results = Vec::new();
        
        for iteration in 1..=args.iterations {
            print!("  Iteration {}/{}... ", iteration, args.iterations);
            
            let result = benchmark_single_model(&model_info, &test_texts).await?;
            iteration_results.push(result);
            
            println!("✓");
        }
        
        // Calculate average results
        let avg_result = average_benchmark_results(&model_info.name, iteration_results);
        results.push(avg_result);
        
        println!();
    }
    
    // Display results
    match args.format.as_str() {
        "json" => {
            println!("{}", serde_json::to_string_pretty(&results)?);
        },
        "csv" => {
            // CSV output implementation
            println!("model,texts_per_second,avg_latency_ms,model_load_time_s,peak_memory_mb,cache_efficiency");
            for result in results {
                println!("{},{},{},{},{},{}", 
                         result.model, result.texts_per_second, 
                         result.avg_latency_ms, result.model_load_time_s,
                         result.peak_memory_mb, result.cache_efficiency);
            }
        },
        _ => {
            let table = Table::new(results);
            println!("{}", table);
        }
    }
    
    Ok(())
}

async fn benchmark_single_model(
    model_info: &crate::models::ModelInfo,
    test_texts: &[String]
) -> Result<BenchmarkResult> {
    let mut generator = OptimizedEmbeddingGenerator::new_with_model(model_info).await?;
    
    let start_time = Instant::now();
    let _embeddings = generator.embed_with_monitoring(test_texts)?;
    let total_time = start_time.elapsed();
    
    let report = generator.get_performance_report();
    
    Ok(BenchmarkResult {
        model: model_info.name.clone(),
        texts_per_second: test_texts.len() as f32 / total_time.as_secs_f32(),
        avg_latency_ms: report.average_time_per_text.as_millis() as f32,
        model_load_time_s: report.model_load_time
            .map(|d| d.as_secs_f32())
            .unwrap_or(0.0),
        peak_memory_mb: report.peak_memory_usage as f32 / 1_048_576.0,
        cache_efficiency: report.cache_efficiency * 100.0,
    })
}

fn generate_test_texts(count: usize) -> Vec<String> {
    let sample_texts = vec![
        "function calculateSum(a, b) { return a + b; }",
        "def process_data(data): return data.strip().upper()",
        "class UserService { authenticate(user) { return user.isValid(); } }",
        "async function fetchData(url) { const response = await fetch(url); return response.json(); }",
        "fn main() { println!(\"Hello, world!\"); }",
        "import React from 'react'; const App = () => <div>Hello</div>;",
        "SELECT * FROM users WHERE active = true AND created_date > '2023-01-01'",
        "public class Calculator { public int add(int a, int b) { return a + b; } }",
    ];
    
    (0..count)
        .map(|i| sample_texts[i % sample_texts.len()].to_string())
        .collect()
}

fn load_sample_texts(file_path: &std::path::Path, max_count: usize) -> Result<Vec<String>> {
    let content = std::fs::read_to_string(file_path)?;
    let lines: Vec<String> = content.lines()
        .filter(|line| !line.trim().is_empty())
        .take(max_count)
        .map(|s| s.to_string())
        .collect();
    
    if lines.is_empty() {
        return Err(anyhow::anyhow!("No valid text found in sample file"));
    }
    
    Ok(lines)
}

fn average_benchmark_results(model_name: &str, results: Vec<BenchmarkResult>) -> BenchmarkResult {
    let count = results.len() as f32;
    
    BenchmarkResult {
        model: model_name.to_string(),
        texts_per_second: results.iter().map(|r| r.texts_per_second).sum::<f32>() / count,
        avg_latency_ms: results.iter().map(|r| r.avg_latency_ms).sum::<f32>() / count,
        model_load_time_s: results.iter().map(|r| r.model_load_time_s).sum::<f32>() / count,
        peak_memory_mb: results.iter().map(|r| r.peak_memory_mb).sum::<f32>() / count,
        cache_efficiency: results.iter().map(|r| r.cache_efficiency).sum::<f32>() / count,
    }
}
```

### 4. Resource Usage Warnings

Add warning system in `src/warnings.rs`:
```rust
use crate::models::ModelInfo;
use tracing::warn;

pub struct ResourceWarnings;

impl ResourceWarnings {
    pub fn check_model_requirements(model_info: &ModelInfo) {
        // Check available memory
        if let Ok(available_memory) = Self::get_available_memory() {
            let required_memory = Self::estimate_model_memory_usage(model_info);
            
            if required_memory > available_memory {
                warn!(
                    "Model '{}' may require {}MB of memory, but only {}MB available. \
                     Consider using a smaller model or increasing system memory.",
                    model_info.name,
                    required_memory / 1_048_576,
                    available_memory / 1_048_576
                );
            }
        }
        
        // Check model download size
        if model_info.size_bytes > 1_000_000_000 { // > 1GB
            warn!(
                "Model '{}' is large ({:.1}GB). Initial download may take significant time.",
                model_info.name,
                model_info.size_bytes as f32 / 1_073_741_824.0
            );
        }
        
        // Model-specific warnings
        match model_info.name.as_str() {
            name if name.contains("gguf") => {
                warn!("GGUF models may have slower inference on CPU. Consider GPU acceleration for better performance.");
            },
            name if name.contains("Qwen3") => {
                warn!("Qwen3 models support instruction-based embeddings. Use --instruction flag for optimal results.");
            },
            _ => {}
        }
    }
    
    fn get_available_memory() -> Result<u64, Box<dyn std::error::Error>> {
        #[cfg(target_os = "linux")]
        {
            let meminfo = std::fs::read_to_string("/proc/meminfo")?;
            for line in meminfo.lines() {
                if line.starts_with("MemAvailable:") {
                    let kb: u64 = line.split_whitespace()
                        .nth(1)
                        .ok_or("Invalid format")?
                        .parse()?;
                    return Ok(kb * 1024);
                }
            }
        }
        
        // Default/fallback
        Ok(8 * 1_073_741_824) // Assume 8GB
    }
    
    fn estimate_model_memory_usage(model_info: &ModelInfo) -> u64 {
        // Rough estimation based on model size and type
        match model_info.backend {
            crate::models::ModelBackend::FastEmbed => {
                // FastEmbed models are typically efficient
                model_info.size_bytes * 2
            },
            crate::models::ModelBackend::Candle => {
                // GGUF models might need additional memory for processing
                model_info.size_bytes * 3
            },
            crate::models::ModelBackend::Custom => {
                // Hugging Face models might need more memory
                model_info.size_bytes * 4
            },
        }
    }
}
```

## Files to Create/Modify
- `src/metrics.rs` - Performance metrics collection
- `src/embeddings.rs` - Optimized embedding generation
- `src/commands/benchmark.rs` - Benchmarking tools
- `src/warnings.rs` - Resource usage warnings
- `src/backends/gguf.rs` - Add optimized loading methods
- `src/backends/huggingface.rs` - Add optimized loading methods
- Update `Cargo.toml` - Add LRU cache dependency

## Acceptance Criteria
- [ ] Performance metrics are collected and reported
- [ ] Model loading is optimized for different backends
- [ ] Batch processing optimizations improve throughput
- [ ] Memory usage is monitored and optimized
- [ ] Resource warnings guide users appropriately
- [ ] Benchmarking tools provide comparative analysis
- [ ] Cache efficiency improves performance for repeated texts
- [ ] Performance reports are detailed and actionable

## Success Validation
```bash
# Run performance benchmarks
cargo run --bin tp -- benchmark --models sentence-transformers/all-MiniLM-L6-v2,nomic-embed-code.Q5_K_S.gguf

# Test optimized embedding generation
cargo run --bin tp -- index --repo . --model Qwen/Qwen3-Embedding-0.6B --limit 100

# Monitor resource usage during operation
cargo run --bin tp -- search "function test" --model nomic-embed-code.Q5_K_S.gguf --limit 50

# Performance tests
cargo test performance_tests
```

## Notes
- Performance optimizations should be measurable and well-documented
- Resource monitoring should be lightweight and not impact performance significantly
- Warnings should be helpful without being excessive
- Benchmarking tools should help users make informed model choices

## Proposed Solution

After analyzing the existing codebase, I propose implementing the performance optimization and monitoring features through the following approach:

### Implementation Strategy

1. **Metrics System**: Create a comprehensive metrics collection system that tracks embedding performance, memory usage, and model load times
2. **Optimized Generator**: Enhance the existing EmbeddingGenerator with caching, optimized batch processing, and resource monitoring
3. **Benchmark Command**: Add a new CLI command `benchmark` that allows users to compare model performance
4. **Resource Warnings**: Implement a warning system that provides guidance based on system resources and model requirements
5. **Test-Driven Development**: Write comprehensive tests first, then implement the features

### Key Features to Implement

- **Performance Metrics Collection**: Track embedding times, throughput, memory usage, and cache hit rates
- **LRU Caching**: Implement embedding caching to avoid redundant computations
- **Optimized Batch Processing**: Dynamically adjust batch sizes based on model type and available memory
- **Resource Monitoring**: Monitor system memory and CPU usage during operations
- **Performance Reporting**: Generate detailed performance reports for analysis
- **Model Comparison Tools**: Allow benchmarking multiple models simultaneously
- **Resource Usage Warnings**: Warn users about potential memory constraints or performance issues

This solution builds upon the existing architecture while adding performance optimization layers that are transparent to existing users but provide valuable insights for performance-conscious use cases.