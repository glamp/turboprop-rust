# Step 24: Implement Hugging Face Model Loading Infrastructure

## Objective
Implement the infrastructure to load and run Hugging Face transformer models directly, preparing for the Qwen/Qwen3-Embedding-0.6B model support.

## Background
Unlike GGUF models, Hugging Face models use the standard transformer architecture and can be loaded using the tokenizers crate and either candle-transformers or direct safetensors loading. This step creates the infrastructure for loading any Hugging Face embedding model.

## Tasks
1. Create Hugging Face model wrapper implementing EmbeddingModel trait
2. Implement model downloading from Hugging Face Hub
3. Create Hugging Face model loader implementing ModelLoader trait
4. Add safetensors support for model weights
5. Implement transformer inference pipeline for embeddings

## Implementation Details

### 1. Hugging Face Model Wrapper Structure
```rust
use candle_core::{Device, Tensor};
use candle_transformers::models::qwen2::Qwen2Model;
use tokenizers::Tokenizer;
use hf_hub::api::tokio::Api;

/// Wrapper for Hugging Face transformer models
pub struct HuggingFaceModelWrapper {
    model: Qwen2Model,  // Or generic transformer model
    tokenizer: Tokenizer,
    device: Device,
    model_name: String,
    dimensions: usize,
    context_length: usize,
    config: ModelConfig,
}

#[derive(Debug, Clone)]
pub struct ModelConfig {
    pub hidden_size: usize,
    pub num_layers: usize,
    pub num_attention_heads: usize,
    pub vocab_size: usize,
    pub max_position_embeddings: usize,
}

#[async_trait]
impl EmbeddingModel for HuggingFaceModelWrapper {
    async fn embed(&mut self, texts: &[&str]) -> Result<Vec<Vec<f32>>> {
        let mut embeddings = Vec::new();
        
        for text in texts {
            let embedding = self.embed_single(text).await?;
            embeddings.push(embedding);
        }
        
        Ok(embeddings)
    }
    
    fn embedding_dimensions(&self) -> usize {
        self.dimensions
    }
    
    fn model_name(&self) -> &str {
        &self.model_name
    }
    
    fn context_length(&self) -> usize {
        self.context_length
    }
    
    fn model_type(&self) -> ModelType {
        ModelType::HuggingFace
    }
}

impl HuggingFaceModelWrapper {
    /// Generate embedding for a single text
    async fn embed_single(&mut self, text: &str) -> Result<Vec<f32>> {
        // Tokenize input
        let encoding = self.tokenizer.encode(text, true)
            .map_err(|e| ModelLoadingError::TokenizerError { source: e.into() })?;
            
        let tokens = encoding.get_ids();
        let attention_mask = encoding.get_attention_mask();
        
        // Convert to tensors
        let input_ids = Tensor::new(tokens, &self.device)?
            .unsqueeze(0)?;  // Add batch dimension
            
        let attention_mask = Tensor::new(attention_mask, &self.device)?
            .unsqueeze(0)?;
            
        // Run inference
        let outputs = self.model.forward(&input_ids, &attention_mask)?;
        
        // Apply mean pooling with attention mask
        let embedding = self.mean_pooling(outputs, &attention_mask)?;
        
        // Convert to Vec<f32>
        let embedding_vec = embedding.squeeze(0)?.to_vec1::<f32>()?;
        
        Ok(embedding_vec)
    }
    
    /// Apply mean pooling to get sentence embedding
    fn mean_pooling(&self, last_hidden_state: Tensor, attention_mask: &Tensor) -> Result<Tensor> {
        // Expand attention mask to match hidden state dimensions
        let mask_expanded = attention_mask
            .unsqueeze(-1)?
            .expand_as(&last_hidden_state)?
            .to_dtype(last_hidden_state.dtype())?;
            
        // Apply mask and sum
        let masked_embeddings = (&last_hidden_state * &mask_expanded)?;
        let summed = masked_embeddings.sum(1)?;
        
        // Sum of attention mask for normalization
        let mask_sum = mask_expanded.sum(1)?;
        let mask_sum = mask_sum.clamp(1e-9, f32::INFINITY)?;  // Avoid division by zero
        
        // Normalize
        let normalized = (summed / mask_sum)?;
        
        Ok(normalized)
    }
}
```

### 2. Hugging Face Loader Implementation
```rust
use hf_hub::api::tokio::Api;
use safetensors::SafeTensors;
use std::collections::HashMap;

pub struct HuggingFaceLoader {
    api: Api,
    client: reqwest::Client,
}

impl Default for HuggingFaceLoader {
    fn default() -> Self {
        Self {
            api: Api::new().unwrap_or_else(|_| {
                // Fallback if HF_TOKEN not set
                Api::new_from_cache().unwrap()
            }),
            client: reqwest::Client::new(),
        }
    }
}

#[async_trait]
impl ModelLoader for HuggingFaceLoader {
    async fn load_model(
        &self,
        model_info: &ModelInfo,
        cache_dir: &Path,
    ) -> Result<Box<dyn EmbeddingModel>> {
        let model_cache_dir = cache_dir.join(&model_info.name.replace("/", "_"));
        tokio::fs::create_dir_all(&model_cache_dir).await?;
        
        // Download model files
        let config_path = self.download_config(&model_info.source, &model_cache_dir).await?;
        let tokenizer_path = self.download_tokenizer(&model_info.source, &model_cache_dir).await?;
        let weights_path = self.download_weights(&model_info.source, &model_cache_dir).await?;
        
        // Load configuration
        let config = self.load_model_config(&config_path)?;
        
        // Load tokenizer
        let tokenizer = Tokenizer::from_file(&tokenizer_path)
            .map_err(|e| ModelLoadingError::TokenizerError { source: e.into() })?;
            
        // Load model weights
        let device = Device::Cpu;  // Start with CPU
        let model = self.load_transformer_model(&weights_path, &config, &device)?;
        
        Ok(Box::new(HuggingFaceModelWrapper {
            model,
            tokenizer,
            device,
            model_name: model_info.name.clone(),
            dimensions: model_info.dimensions,
            context_length: model_info.metadata.context_length.unwrap_or(2048),
            config,
        }))
    }
    
    fn can_load(&self, model_type: ModelType) -> bool {
        matches!(model_type, ModelType::HuggingFace)
    }
}

impl HuggingFaceLoader {
    /// Download model configuration file
    async fn download_config(&self, model_id: &str, cache_dir: &Path) -> Result<PathBuf> {
        let config_path = cache_dir.join("config.json");
        
        if !config_path.exists() {
            tracing::info!("Downloading config for {}", model_id);
            
            let repo = self.api.model(model_id.to_string());
            let config_url = repo.url("config.json");
            
            let response = self.client.get(&config_url).send().await
                .map_err(|e| ModelLoadingError::DownloadError {
                    url: config_url.clone(),
                    source: e.into(),
                })?;
                
            let bytes = response.bytes().await
                .map_err(|e| ModelLoadingError::DownloadError {
                    url: config_url.clone(),
                    source: e.into(),
                })?;
                
            tokio::fs::write(&config_path, bytes).await?;
        }
        
        Ok(config_path)
    }
    
    /// Download tokenizer files
    async fn download_tokenizer(&self, model_id: &str, cache_dir: &Path) -> Result<PathBuf> {
        let tokenizer_path = cache_dir.join("tokenizer.json");
        
        if !tokenizer_path.exists() {
            tracing::info!("Downloading tokenizer for {}", model_id);
            
            let repo = self.api.model(model_id.to_string());
            
            // Try tokenizer.json first, fall back to tokenizer_config.json
            let tokenizer_files = ["tokenizer.json", "tokenizer_config.json"];
            
            for file in &tokenizer_files {
                if let Ok(url) = repo.url(file) {
                    if let Ok(response) = self.client.get(&url).send().await {
                        if response.status().is_success() {
                            let bytes = response.bytes().await?;
                            tokio::fs::write(&tokenizer_path, bytes).await?;
                            break;
                        }
                    }
                }
            }
            
            if !tokenizer_path.exists() {
                return Err(ModelLoadingError::DownloadError {
                    url: format!("{}/tokenizer.json", model_id),
                    source: anyhow::anyhow!("Could not download tokenizer files"),
                }.into());
            }
        }
        
        Ok(tokenizer_path)
    }
    
    /// Download model weights (safetensors format)
    async fn download_weights(&self, model_id: &str, cache_dir: &Path) -> Result<PathBuf> {
        let weights_path = cache_dir.join("model.safetensors");
        
        if !weights_path.exists() {
            tracing::info!("Downloading weights for {}", model_id);
            
            let repo = self.api.model(model_id.to_string());
            
            // Try different weight file names
            let weight_files = [
                "model.safetensors",
                "pytorch_model.bin",
                "model-00001-of-00001.safetensors"
            ];
            
            for file in &weight_files {
                if let Ok(url) = repo.url(file) {
                    if let Ok(response) = self.client.get(&url).send().await {
                        if response.status().is_success() {
                            // Download with progress bar for large files
                            self.download_large_file(&url, &weights_path).await?;
                            break;
                        }
                    }
                }
            }
            
            if !weights_path.exists() {
                return Err(ModelLoadingError::DownloadError {
                    url: format!("{}/model.safetensors", model_id),
                    source: anyhow::anyhow!("Could not download model weights"),
                }.into());
            }
        }
        
        Ok(weights_path)
    }
    
    /// Download large file with progress tracking
    async fn download_large_file(&self, url: &str, path: &Path) -> Result<()> {
        use indicatif::{ProgressBar, ProgressStyle};
        use futures::StreamExt;
        
        let response = self.client.get(url).send().await?;
        let total_size = response.content_length().unwrap_or(0);
        
        let pb = ProgressBar::new(total_size);
        pb.set_style(ProgressStyle::default_bar()
            .template("{spinner:.green} [{elapsed_precise}] [{wide_bar:.cyan/blue}] {bytes}/{total_bytes} ({eta})")
            .unwrap()
            .progress_chars("#>-"));
            
        let mut file = tokio::fs::File::create(path).await?;
        let mut downloaded = 0u64;
        let mut stream = response.bytes_stream();
        
        while let Some(chunk) = stream.next().await {
            let chunk = chunk?;
            tokio::io::AsyncWriteExt::write_all(&mut file, &chunk).await?;
            downloaded += chunk.len() as u64;
            pb.set_position(downloaded);
        }
        
        pb.finish_with_message("Download completed");
        Ok(())
    }
    
    /// Load model configuration from JSON
    fn load_model_config(&self, path: &Path) -> Result<ModelConfig> {
        let config_str = std::fs::read_to_string(path)?;
        let config_json: serde_json::Value = serde_json::from_str(&config_str)?;
        
        Ok(ModelConfig {
            hidden_size: config_json["hidden_size"].as_u64().unwrap_or(768) as usize,
            num_layers: config_json["num_hidden_layers"].as_u64().unwrap_or(12) as usize,
            num_attention_heads: config_json["num_attention_heads"].as_u64().unwrap_or(12) as usize,
            vocab_size: config_json["vocab_size"].as_u64().unwrap_or(32000) as usize,
            max_position_embeddings: config_json["max_position_embeddings"].as_u64().unwrap_or(2048) as usize,
        })
    }
    
    /// Load transformer model from weights
    fn load_transformer_model(
        &self,
        weights_path: &Path,
        config: &ModelConfig,
        device: &Device,
    ) -> Result<Qwen2Model> {
        // This is a placeholder - actual implementation depends on the specific
        // model architecture and safetensors loading
        
        // For now, return an error indicating this needs specific implementation
        Err(ModelLoadingError::InitializationError {
            source: anyhow::anyhow!("Transformer model loading not yet implemented - needs architecture-specific code")
        }.into())
    }
}
```

### 3. Update Model Factory
```rust
impl Default for ModelFactory {
    fn default() -> Self {
        Self {
            loaders: vec![
                Box::new(FastEmbedLoader),
                Box::new(GgufLoader::default()),
                Box::new(HuggingFaceLoader::default()),  // Add HF loader
            ],
        }
    }
}
```

### 4. Add safetensors Dependency
Update Cargo.toml:
```toml
safetensors = "0.4"
```

## Test Coverage
Add tests for:
- Hugging Face API integration
- Model file downloading simulation
- Configuration parsing
- Tokenizer loading and validation
- Factory integration with HF loader

## Acceptance Criteria
- [ ] HuggingFaceModelWrapper implements EmbeddingModel trait
- [ ] HuggingFaceLoader implements ModelLoader trait with HF Hub integration
- [ ] Model factory includes Hugging Face loader
- [ ] Progress tracking for large model downloads
- [ ] Configuration parsing from config.json files
- [ ] Tokenizer loading with fallback options
- [ ] safetensors dependency added
- [ ] Comprehensive test coverage for HF infrastructure
- [ ] `cargo build` succeeds (may have unimplemented parts marked as TODO)
- [ ] Tests pass for implemented functionality

## Files to Create/Modify
- `src/models.rs` - Add HuggingFaceModelWrapper and HuggingFaceLoader
- `src/models.rs` - Update ModelFactory to include HF loader
- `src/models.rs` - Add test cases for HF infrastructure
- `Cargo.toml` - Add safetensors and any missing dependencies

## Success Validation
```bash
cargo build  # Should succeed with warnings about unimplemented parts
cargo test models::test_huggingface_loader
cargo test models::test_model_factory_with_hf
cargo test models::test_config_parsing
```

## Notes
This step creates the infrastructure but leaves the actual transformer model loading as TODO because it requires specific implementation for the Qwen3 architecture. The next step will implement the specific Qwen/Qwen3-Embedding-0.6B model support.

The implementation assumes the model uses safetensors format, which is becoming standard for Hugging Face models. PyTorch .bin format support can be added if needed.