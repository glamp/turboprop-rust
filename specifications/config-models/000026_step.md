# Step 26: Update EmbeddingGenerator to Use Unified Model Interface

## Objective
Refactor the EmbeddingGenerator to use the new unified model interface, enabling it to work seamlessly with fastembed, GGUF, and Hugging Face models through the trait-based system.

## Background
Currently, EmbeddingGenerator in src/embeddings.rs is tightly coupled to fastembed. With the new unified model interface, we need to refactor it to use the ModelFactory and EmbeddingModel trait, making it model-agnostic while maintaining backward compatibility.

## Tasks
1. Refactor EmbeddingGenerator to use EmbeddingModel trait
2. Update initialization to use ModelFactory
3. Maintain backward compatibility with existing APIs
4. Update batch processing to work with any model type
5. Add model-specific optimizations and configurations
6. Update error handling for different model types

## Implementation Details

### 1. Refactor EmbeddingGenerator Structure
```rust
use crate::models::{ModelFactory, EmbeddingModel, ModelManager};

/// Main embedding generator that handles model loading and text embedding
pub struct EmbeddingGenerator {
    model: Box<dyn EmbeddingModel>,
    config: EmbeddingConfig,
    model_factory: ModelFactory,
}

impl std::fmt::Debug for EmbeddingGenerator {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("EmbeddingGenerator")
            .field("config", &self.config)
            .field("model_name", &self.model.model_name())
            .field("model_type", &self.model.model_type())
            .field("dimensions", &self.model.embedding_dimensions())
            .finish()
    }
}
```

### 2. Update EmbeddingGenerator Initialization
```rust
impl EmbeddingGenerator {
    /// Initialize a new embedding generator with the specified configuration
    pub async fn new(config: EmbeddingConfig) -> Result<Self> {
        info!(
            "Initializing embedding generator with model: {}",
            config.model_name
        );

        // Get model info from ModelManager
        let model_info = ModelManager::get_model_info(&config.model_name)
            .ok_or_else(|| {
                TurboPropError::other(format!("Unknown model: {}", config.model_name))
            })?;

        // Ensure cache directory exists
        if !config.cache_dir.exists() {
            std::fs::create_dir_all(&config.cache_dir).with_context(|| {
                format!(
                    "Failed to create cache directory: {}",
                    config.cache_dir.display()
                )
            })?;
            debug!("Created cache directory: {}", config.cache_dir.display());
        }

        // Load model using factory
        let model_factory = ModelFactory::default();
        info!("Loading model (this may download the model on first use)...");
        
        let model = model_factory
            .load_model(&model_info, &config.cache_dir)
            .await
            .with_context(|| {
                format!("Failed to initialize model: {}", config.model_name)
            })?;

        info!("Model loaded successfully");

        // Verify dimensions match configuration
        let actual_dimensions = model.embedding_dimensions();
        if actual_dimensions != config.embedding_dimensions {
            warn!(
                "Model dimensions ({}) don't match config ({}), updating config",
                actual_dimensions, config.embedding_dimensions
            );
        }

        let mut updated_config = config;
        updated_config.embedding_dimensions = actual_dimensions;

        Ok(Self {
            model,
            config: updated_config,
            model_factory,
        })
    }

    /// Create embedding generator from model name (convenience method)
    pub async fn from_model_name(model_name: &str) -> Result<Self> {
        let config = EmbeddingConfig::with_model(model_name);
        Self::new(config).await
    }
}
```

### 3. Update Embedding Generation Methods
```rust
impl EmbeddingGenerator {
    /// Generate embeddings for a single text chunk
    pub async fn embed_single(&mut self, text: &str) -> Result<Vec<f32>> {
        if text.is_empty() {
            return Ok(vec![0.0; self.config.embedding_dimensions]);
        }

        let embeddings = self.model.embed(&[text]).await.with_context(|| {
            format!(
                "Failed to generate embedding for text: {}",
                if text.len() > ERROR_MESSAGE_TEXT_PREVIEW_LENGTH {
                    &text[..ERROR_MESSAGE_TEXT_PREVIEW_LENGTH]
                } else {
                    text
                }
            )
        })?;

        embeddings.into_iter().next().ok_or_else(|| {
            TurboPropError::other("Failed to generate embedding: no output for input text").into()
        })
    }

    /// Generate embeddings for multiple text chunks in batches
    pub async fn embed_batch(&mut self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        if texts.is_empty() {
            return Ok(Vec::new());
        }

        let text_refs: Vec<&str> = texts.iter().map(|s| s.as_str()).collect();
        let mut all_embeddings = Vec::with_capacity(texts.len());

        // Process in batches to avoid memory issues with large inputs
        for chunk in text_refs.chunks(self.config.batch_size) {
            debug!("Processing batch of {} texts", chunk.len());

            let batch_embeddings = self.model.embed(chunk).await.with_context(|| {
                format!(
                    "Failed to generate embeddings for batch of {} texts",
                    chunk.len()
                )
            })?;

            all_embeddings.extend(batch_embeddings);
        }

        debug!("Generated {} embeddings total", all_embeddings.len());
        Ok(all_embeddings)
    }

    /// Generate embeddings for multiple text chunks with optimized batching
    pub async fn embed_batch_optimized(&mut self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        if texts.is_empty() {
            return Ok(Vec::new());
        }

        // Calculate optimal batch size based on model type and system resources
        let optimal_batch_size = self.calculate_optimal_batch_size(texts).await;
        debug!(
            "Using optimal batch size: {} for {} texts",
            optimal_batch_size,
            texts.len()
        );

        let text_refs: Vec<&str> = texts.iter().map(|s| s.as_str()).collect();
        let mut all_embeddings = Vec::with_capacity(texts.len());

        // Process with optimal batching
        let start_time = std::time::Instant::now();
        for (batch_idx, chunk) in text_refs.chunks(optimal_batch_size).enumerate() {
            let batch_start = std::time::Instant::now();

            let batch_embeddings = self.model.embed(chunk).await.with_context(|| {
                format!(
                    "Failed to generate embeddings for batch {} of {} texts",
                    batch_idx + 1,
                    chunk.len()
                )
            })?;

            all_embeddings.extend(batch_embeddings);

            let batch_time = batch_start.elapsed();
            debug!(
                "Batch {} completed in {:.2}ms ({:.1} texts/sec)",
                batch_idx + 1,
                batch_time.as_secs_f64() * 1000.0,
                chunk.len() as f64 / batch_time.as_secs_f64()
            );
        }

        let total_time = start_time.elapsed();
        info!(
            "Generated {} embeddings in {:.2}s ({:.1} texts/sec)",
            all_embeddings.len(),
            total_time.as_secs_f64(),
            texts.len() as f64 / total_time.as_secs_f64()
        );

        Ok(all_embeddings)
    }
}
```

### 4. Model-Specific Optimizations
```rust
impl EmbeddingGenerator {
    /// Calculate optimal batch size based on model type and characteristics
    async fn calculate_optimal_batch_size(&self, texts: &[String]) -> usize {
        if texts.is_empty() {
            return self.config.batch_size;
        }

        // Get base batch size considering model type
        let base_batch_size = match self.model.model_type() {
            ModelType::FastEmbed => self.config.batch_size,
            ModelType::Gguf => {
                // GGUF models might need smaller batches due to memory constraints
                self.config.batch_size / 2
            }
            ModelType::HuggingFace => {
                // Large transformer models need careful memory management
                match self.config.model_name.as_str() {
                    "Qwen/Qwen3-Embedding-0.6B" => self.config.batch_size / 4, // Large model
                    _ => self.config.batch_size / 2,
                }
            }
        };

        // Calculate average text length
        let avg_length = texts.iter().map(|t| t.len()).sum::<usize>() / texts.len();

        // Adjust batch size based on text length
        let length_adjusted = if avg_length > 1000 {
            base_batch_size / 2
        } else if avg_length < 100 {
            base_batch_size * 2
        } else {
            base_batch_size
        };

        // Apply memory-based scaling
        if let Ok(mem_info) = sys_info::mem_info() {
            let available_gb = mem_info.avail / 1024 / 1024;
            let memory_factor = if available_gb >= 8 {
                1.5
            } else if available_gb >= 4 {
                1.0
            } else {
                0.5
            };

            let memory_adjusted = (length_adjusted as f64 * memory_factor) as usize;
            memory_adjusted.clamp(1, 256)
        } else {
            length_adjusted.clamp(1, 128)
        }
    }

    /// Get model information
    pub fn model_info(&self) -> ModelInfo {
        ModelInfo {
            name: self.model.model_name().to_string(),
            model_type: self.model.model_type(),
            dimensions: self.model.embedding_dimensions(),
            context_length: self.model.context_length(),
        }
    }

    /// Get the embedding dimensions for this model
    pub fn embedding_dimensions(&self) -> usize {
        self.model.embedding_dimensions()
    }

    /// Get the model name being used
    pub fn model_name(&self) -> &str {
        self.model.model_name()
    }

    /// Get the model type
    pub fn model_type(&self) -> ModelType {
        self.model.model_type()
    }

    /// Get the maximum context length
    pub fn context_length(&self) -> usize {
        self.model.context_length()
    }
}

#[derive(Debug)]
pub struct ModelInfo {
    pub name: String,
    pub model_type: ModelType,
    pub dimensions: usize,
    pub context_length: usize,
}
```

### 5. Update EmbeddingConfig for New Models
```rust
impl EmbeddingConfig {
    /// Create a new configuration with a custom model name
    pub fn with_model(model_name: impl Into<String>) -> Self {
        let model_name = model_name.into();

        // Look up model dimensions from available models
        let embedding_dimensions = ModelManager::get_available_models()
            .iter()
            .find(|model| model.name == model_name)
            .map(|model| model.dimensions)
            .unwrap_or_else(|| {
                warn!("Unknown model {}, using default dimensions", model_name);
                DEFAULT_EMBEDDING_DIMENSIONS
            });

        Self {
            model_name,
            embedding_dimensions,
            ..Default::default()
        }
    }

    /// Validate configuration against available models
    pub fn validate(&self) -> Result<()> {
        if !ModelManager::get_available_models()
            .iter()
            .any(|model| model.name == self.model_name)
        {
            return Err(anyhow::anyhow!("Unknown model: {}", self.model_name));
        }
        Ok(())
    }
}
```

### 6. Update Error Handling
```rust
impl From<ModelLoadingError> for TurboPropError {
    fn from(error: ModelLoadingError) -> Self {
        match error {
            ModelLoadingError::UnsupportedModelType { model_type } => {
                TurboPropError::other(format!("Unsupported model type: {:?}", model_type))
            }
            ModelLoadingError::DownloadError { url, source } => {
                TurboPropError::other(format!("Failed to download from {}: {}", url, source))
            }
            ModelLoadingError::InitializationError { source } => {
                TurboPropError::other(format!("Model initialization failed: {}", source))
            }
            ModelLoadingError::TokenizerError { source } => {
                TurboPropError::other(format!("Tokenizer error: {}", source))
            }
            ModelLoadingError::InvalidFormat { message } => {
                TurboPropError::other(format!("Invalid model format: {}", message))
            }
        }
    }
}
```

## Test Coverage
Update existing tests and add new ones:
- EmbeddingGenerator with different model types
- Batch processing with various models
- Error handling for unsupported models
- Model switching and reinitialization
- Memory usage with large models
- Performance comparison between model types

## Acceptance Criteria
- [ ] EmbeddingGenerator refactored to use EmbeddingModel trait
- [ ] Initialization works with ModelFactory
- [ ] All existing APIs maintain backward compatibility
- [ ] Batch processing works with all model types
- [ ] Model-specific optimizations implemented
- [ ] Error handling covers all model loading scenarios
- [ ] Memory usage is reasonable for all model types
- [ ] Performance is maintained or improved
- [ ] Comprehensive test coverage
- [ ] All existing tests pass

## Files to Modify
- `src/embeddings.rs` - Complete refactor of EmbeddingGenerator
- `src/embeddings.rs` - Update EmbeddingConfig and error handling
- `src/embeddings.rs` - Update all test cases

## Success Validation
```bash
# Build should succeed
cargo build

# All existing tests should pass
cargo test embeddings::

# New functionality tests
cargo test embeddings::test_model_factory_integration
cargo test embeddings::test_different_model_types
cargo test embeddings::test_batch_optimization
```

## Notes
This step maintains full backward compatibility while enabling the new model types. Existing code using EmbeddingGenerator should continue to work without changes, but will now have access to the new models when specified in configuration.

The batch size calculations are conservative to avoid memory issues with large models. They can be tuned based on actual performance testing.