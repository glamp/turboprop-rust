# Step 19: Add Dependencies for Custom Model Loading

## Objective
Add the necessary dependencies to support custom model loading beyond fastembed, specifically for GGUF models and Hugging Face transformers.

## Background
The specification requires support for two new embedding models that are not available in fastembed:
- `nomic-embed-code.Q5_K_S.gguf` - A GGUF quantized model requiring custom loading
- `Qwen/Qwen3-Embedding-0.6B` - A Hugging Face model requiring transformer loading

This step adds the foundational dependencies needed for both model types.

## Tasks
1. Add candle-core and related dependencies for GGUF model support
2. Add tokenizers and transformers dependencies for Hugging Face models
3. Add HTTP client dependencies for model downloading
4. Update Cargo.toml with version constraints and features
5. Verify dependencies build correctly

## Implementation Details

### Dependencies to Add

#### GGUF and ML Framework Support
```toml
# Candle framework for ML model loading (supports GGUF)
candle-core = "0.7"
candle-transformers = "0.7"
candle-nn = "0.7"

# For GGUF file format support
gguf-file = "0.2"
```

#### Tokenizer and NLP Support
```toml
# Tokenizers for Hugging Face models
tokenizers = "0.19"
hf-hub = "0.3"  # For downloading from Hugging Face
```

#### Enhanced HTTP Client
```toml
# Enhanced reqwest features for model downloads
reqwest = { version = "0.11", features = ["json", "stream"] }
```

#### Async and Concurrency
```toml
# Enhanced tokio features
tokio = { version = "1.0", features = ["full", "fs"] }
```

### Version Compatibility
- Ensure all ML dependencies are compatible with each other
- Use specific version ranges to avoid breaking changes
- Test compatibility with existing fastembed dependency

### Feature Flags (Optional)
Consider adding feature flags for optional model support:
```toml
[features]
default = ["fastembed-models"]
fastembed-models = ["fastembed"]
custom-models = ["candle-core", "candle-transformers", "tokenizers"]
gguf-models = ["gguf-file"]
hf-models = ["hf-hub"]
```

## Acceptance Criteria
- [ ] All new dependencies added to Cargo.toml
- [ ] Dependencies are compatible with existing ones
- [ ] `cargo build` succeeds without conflicts
- [ ] `cargo test` passes all existing tests
- [ ] Dependencies support the required model formats
- [ ] No breaking changes to existing functionality
- [ ] Documentation updated if feature flags are added

## Files to Modify
- `Cargo.toml` - Add new dependencies

## Success Validation
```bash
# Verify build succeeds
cargo build

# Verify tests still pass
cargo test

# Check dependency tree for conflicts
cargo tree

# Verify specific dependencies are available
cargo doc --open  # Check that new crates are documented
```

## Notes
This step only adds dependencies and verifies compatibility. No functional changes to the codebase are made. The actual model loading implementation will be added in subsequent steps.

## Dependencies Research
Based on research:
- candle-core is a modern Rust ML framework that supports GGUF
- tokenizers crate provides Hugging Face tokenizer compatibility
- gguf-file provides direct GGUF format parsing
- hf-hub enables direct downloads from Hugging Face Hub

## Risk Mitigation
- Use specific version ranges to avoid future breaking changes
- Test build on different platforms if possible
- Consider optional feature flags to keep binary size manageable
- Ensure licensing compatibility with existing MIT/Apache-2.0 license