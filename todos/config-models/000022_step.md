# Step 22: Implement GGUF Model Loading Infrastructure

## Objective
Implement the infrastructure to load and run GGUF format models using the candle framework, preparing for the nomic-embed-code.Q5_K_S.gguf model support.

## Background
GGUF (GPT-Generated Unified Format) is a file format for storing models that's designed for efficient inference. The nomic-embed-code model is available in this format and requires specialized loading and inference code using the candle ML framework.

## Tasks
1. Create GGUF model wrapper implementing EmbeddingModel trait
2. Implement GGUF file downloading and caching
3. Create GGUF model loader implementing ModelLoader trait
4. Add tokenization support for GGUF models
5. Implement inference pipeline for embeddings

## Implementation Details

### 1. GGUF Model Wrapper Structure
```rust
use candle_core::{Device, Tensor, DType};
use candle_transformers::models::bert::BertModel;
use tokenizers::Tokenizer;

/// Wrapper for GGUF models loaded with candle
pub struct GgufModelWrapper {
    model: BertModel,  // or appropriate model type
    tokenizer: Tokenizer,
    device: Device,
    model_name: String,
    dimensions: usize,
    context_length: usize,
}

#[async_trait]
impl EmbeddingModel for GgufModelWrapper {
    async fn embed(&mut self, texts: &[&str]) -> Result<Vec<Vec<f32>>> {
        let mut embeddings = Vec::new();
        
        for text in texts {
            // Tokenize text
            let encoding = self.tokenizer.encode(text, true)
                .map_err(|e| ModelLoadingError::TokenizerError { source: e.into() })?;
                
            let tokens = encoding.get_ids();
            let tensor = Tensor::new(tokens, &self.device)
                .map_err(|e| ModelLoadingError::InitializationError { source: e.into() })?;
                
            // Run inference
            let output = self.model.forward(&tensor)
                .map_err(|e| ModelLoadingError::InitializationError { source: e.into() })?;
                
            // Extract embeddings (typically from last hidden state)
            let embedding = self.extract_embedding(output)?;
            embeddings.push(embedding);
        }
        
        Ok(embeddings)
    }
    
    fn embedding_dimensions(&self) -> usize {
        self.dimensions
    }
    
    fn model_name(&self) -> &str {
        &self.model_name
    }
    
    fn context_length(&self) -> usize {
        self.context_length
    }
    
    fn model_type(&self) -> ModelType {
        ModelType::Gguf
    }
}

impl GgufModelWrapper {
    /// Extract embedding from model output (implementation depends on model architecture)
    fn extract_embedding(&self, output: Tensor) -> Result<Vec<f32>> {
        // For embedding models, typically take mean of last hidden states
        // Implementation will depend on specific model architecture
        let embedding = output.mean(1)?  // Mean over sequence dimension
            .to_vec1::<f32>()
            .map_err(|e| ModelLoadingError::InitializationError { source: e.into() })?;
            
        Ok(embedding)
    }
}
```

### 2. GGUF Loader Implementation
```rust
use reqwest::Client;
use std::io::Write;

pub struct GgufLoader {
    client: Client,
}

impl Default for GgufLoader {
    fn default() -> Self {
        Self {
            client: Client::new(),
        }
    }
}

#[async_trait]
impl ModelLoader for GgufLoader {
    async fn load_model(
        &self,
        model_info: &ModelInfo,
        cache_dir: &Path,
    ) -> Result<Box<dyn EmbeddingModel>> {
        // Create model-specific cache directory
        let model_cache_dir = cache_dir.join(&model_info.name.replace("/", "_"));
        tokio::fs::create_dir_all(&model_cache_dir).await?;
        
        // Download model file if not cached
        let model_path = model_cache_dir.join("model.gguf");
        if !model_path.exists() {
            self.download_model(&model_info.source, &model_path).await?;
        }
        
        // Download tokenizer if needed
        let tokenizer_path = model_cache_dir.join("tokenizer.json");
        if !tokenizer_path.exists() {
            self.download_tokenizer(model_info, &tokenizer_path).await?;
        }
        
        // Load model and tokenizer
        let device = Device::Cpu; // Start with CPU, could add GPU support later
        
        let model = self.load_gguf_model(&model_path, &device)?;
        let tokenizer = Tokenizer::from_file(&tokenizer_path)
            .map_err(|e| ModelLoadingError::TokenizerError { source: e.into() })?;
            
        Ok(Box::new(GgufModelWrapper {
            model,
            tokenizer,
            device,
            model_name: model_info.name.clone(),
            dimensions: model_info.dimensions,
            context_length: model_info.metadata.context_length.unwrap_or(8192),
        }))
    }
    
    fn can_load(&self, model_type: ModelType) -> bool {
        matches!(model_type, ModelType::Gguf)
    }
}

impl GgufLoader {
    /// Download model file from URL
    async fn download_model(&self, url: &str, path: &Path) -> Result<()> {
        tracing::info!("Downloading GGUF model from {}", url);
        
        let response = self.client.get(url).send().await
            .map_err(|e| ModelLoadingError::DownloadError {
                url: url.to_string(),
                source: e.into(),
            })?;
            
        if !response.status().is_success() {
            return Err(ModelLoadingError::DownloadError {
                url: url.to_string(),
                source: anyhow::anyhow!("HTTP {}", response.status()),
            }.into());
        }
        
        let bytes = response.bytes().await
            .map_err(|e| ModelLoadingError::DownloadError {
                url: url.to_string(),
                source: e.into(),
            })?;
            
        let mut file = std::fs::File::create(path)?;
        file.write_all(&bytes)?;
        
        tracing::info!("GGUF model downloaded to {:?}", path);
        Ok(())
    }
    
    /// Download tokenizer (may need to infer from model or use default)
    async fn download_tokenizer(&self, model_info: &ModelInfo, path: &Path) -> Result<()> {
        // For nomic models, we might need to use a specific tokenizer
        // This is model-specific and may require research
        let tokenizer_url = self.get_tokenizer_url(model_info)?;
        
        let response = self.client.get(&tokenizer_url).send().await
            .map_err(|e| ModelLoadingError::DownloadError {
                url: tokenizer_url.clone(),
                source: e.into(),
            })?;
            
        let bytes = response.bytes().await
            .map_err(|e| ModelLoadingError::DownloadError {
                url: tokenizer_url,
                source: e.into(),
            })?;
            
        let mut file = std::fs::File::create(path)?;
        file.write_all(&bytes)?;
        
        Ok(())
    }
    
    /// Get tokenizer URL for a model (model-specific logic)
    fn get_tokenizer_url(&self, model_info: &ModelInfo) -> Result<String> {
        match model_info.name.as_str() {
            "nomic-embed-code.Q5_K_S.gguf" => {
                // Research needed: find the correct tokenizer for nomic-embed-code
                Ok("https://huggingface.co/nomic-ai/nomic-embed-text-v1/resolve/main/tokenizer.json".to_string())
            },
            _ => Err(anyhow::anyhow!("Unknown tokenizer for model {}", model_info.name)),
        }
    }
    
    /// Load GGUF model using candle
    fn load_gguf_model(&self, path: &Path, device: &Device) -> Result<BertModel> {
        // This is a placeholder - actual implementation depends on candle's GGUF support
        // and the specific model architecture
        
        // For now, return an error indicating this needs implementation
        Err(ModelLoadingError::InitializationError {
            source: anyhow::anyhow!("GGUF loading not yet implemented - needs model architecture research")
        }.into())
    }
}
```

### 3. Update Model Factory
```rust
impl Default for ModelFactory {
    fn default() -> Self {
        Self {
            loaders: vec![
                Box::new(FastEmbedLoader),
                Box::new(GgufLoader::default()),  // Add GGUF loader
            ],
        }
    }
}
```

### 4. Add Progress Tracking for Downloads
```rust
use indicatif::{ProgressBar, ProgressStyle};

impl GgufLoader {
    /// Download with progress bar
    async fn download_with_progress(&self, url: &str, path: &Path) -> Result<()> {
        let response = self.client.get(url).send().await?;
        let total_size = response.content_length().unwrap_or(0);
        
        let pb = ProgressBar::new(total_size);
        pb.set_style(ProgressStyle::default_bar()
            .template("{spinner:.green} [{elapsed_precise}] [{wide_bar:.cyan/blue}] {bytes}/{total_bytes} ({eta})")
            .unwrap()
            .progress_chars("#>-"));
            
        let mut file = tokio::fs::File::create(path).await?;
        let mut downloaded = 0u64;
        let mut stream = response.bytes_stream();
        
        use futures::StreamExt;
        while let Some(chunk) = stream.next().await {
            let chunk = chunk?;
            file.write_all(&chunk).await?;
            downloaded += chunk.len() as u64;
            pb.set_position(downloaded);
        }
        
        pb.finish_with_message("Download completed");
        Ok(())
    }
}
```

## Test Coverage
Add tests for:
- GGUF file download simulation
- Model loading error cases
- Factory integration with GGUF loader
- Tokenizer loading and validation

## Acceptance Criteria
- [ ] GgufModelWrapper implements EmbeddingModel trait
- [ ] GgufLoader implements ModelLoader trait with download capability  
- [ ] Model factory includes GGUF loader
- [ ] Progress tracking for model downloads
- [ ] Error handling for download failures and invalid models
- [ ] Comprehensive test coverage for GGUF loading infrastructure
- [ ] `cargo build` succeeds (may have unimplemented parts marked as TODO)
- [ ] Tests pass for implemented functionality

## Files to Create/Modify
- `src/models.rs` - Add GgufModelWrapper and GgufLoader
- `src/models.rs` - Update ModelFactory to include GGUF loader
- `src/models.rs` - Add test cases for GGUF infrastructure

## Success Validation
```bash
cargo build  # Should succeed with warnings about unimplemented parts
cargo test models::test_gguf_loader
cargo test models::test_model_factory_with_gguf
```

## Notes
This step creates the infrastructure but leaves the actual GGUF model loading as TODO because it requires research into the specific model architecture and candle's GGUF support. The next step will implement the specific nomic-embed-code model support.

The tokenizer URL is a placeholder and needs research to find the correct tokenizer for nomic-embed-code models.