# Step 23: Add Nomic Embed Code Model Support

## Objective
Complete the implementation of nomic-embed-code.Q5_K_S.gguf model support by implementing the actual model loading, inference, and determining the correct model architecture and dimensions.

## Background
This step builds on the GGUF infrastructure from Step 22 to implement the specific nomic-embed-code model. Based on research, this is a 7B parameter code embedding model trained on multiple programming languages, but the exact architecture and dimensions need to be determined through implementation and testing.

## Tasks
1. Research and implement the correct model architecture for nomic-embed-code
2. Determine the actual embedding dimensions through testing
3. Find and configure the correct tokenizer
4. Implement the complete inference pipeline
5. Add comprehensive testing with real model files
6. Optimize performance for code embedding use cases

## Implementation Details

### 1. Research Model Architecture
First, investigate the actual model architecture by examining the GGUF file:
```rust
use gguf_file::{GgufFile, GgmlDType};

impl GgufLoader {
    /// Inspect GGUF file to determine architecture
    fn inspect_gguf_model(&self, path: &Path) -> Result<ModelArchitectureInfo> {
        let gguf = GgufFile::open(path)
            .map_err(|e| ModelLoadingError::InvalidFormat {
                message: format!("Failed to open GGUF file: {}", e)
            })?;
            
        // Extract architecture information from metadata
        let metadata = gguf.metadata();
        let architecture = metadata.get("general.architecture")
            .ok_or_else(|| ModelLoadingError::InvalidFormat {
                message: "Missing architecture in GGUF metadata".to_string()
            })?;
            
        let embedding_length = metadata.get("general.embedding_length")
            .and_then(|val| val.as_u32().map(|v| v as usize));
            
        let context_length = metadata.get("general.context_length")
            .and_then(|val| val.as_u32().map(|v| v as usize))
            .unwrap_or(8192);
            
        Ok(ModelArchitectureInfo {
            architecture: architecture.as_string().unwrap_or("unknown".to_string()),
            embedding_length,
            context_length,
            quantization: self.detect_quantization(path)?,
        })
    }
}

#[derive(Debug)]
struct ModelArchitectureInfo {
    architecture: String,
    embedding_length: Option<usize>,
    context_length: usize,
    quantization: String,
}
```

### 2. Implement Model Loading with Candle
```rust
use candle_core::{quantized::gguf_file, Device, Tensor};
use candle_transformers::models::qwen2::Qwen2Model;  // Based on research that it's based on Qwen2

impl GgufLoader {
    /// Load nomic-embed-code GGUF model
    fn load_nomic_embed_code(&self, path: &Path, device: &Device) -> Result<NomicEmbedCodeModel> {
        // Load GGUF file
        let mut file = std::fs::File::open(path)?;
        let model = gguf_file::Content::read(&mut file)
            .map_err(|e| ModelLoadingError::InvalidFormat {
                message: format!("Failed to read GGUF file: {}", e)
            })?;
            
        // Based on research, nomic-embed-code is based on Qwen2
        // We need to adapt it for embedding extraction
        let config = self.create_model_config(&model)?;
        let weights = model.tensor_data;
        
        Ok(NomicEmbedCodeModel::new(config, weights, device.clone())?)
    }
    
    fn create_model_config(&self, model: &gguf_file::Content) -> Result<Qwen2Config> {
        // Extract configuration parameters from GGUF metadata
        let metadata = &model.metadata;
        
        let vocab_size = metadata.get("tokenizer.ggml.vocab_size")
            .and_then(|v| v.as_u32())
            .unwrap_or(32000) as usize;
            
        let hidden_size = metadata.get("qwen2.embedding_length")
            .and_then(|v| v.as_u32())
            .unwrap_or(768) as usize;  // Default, will be verified
            
        let num_attention_heads = metadata.get("qwen2.attention.head_count")
            .and_then(|v| v.as_u32())
            .unwrap_or(12) as usize;
            
        let num_hidden_layers = metadata.get("qwen2.block_count")
            .and_then(|v| v.as_u32())
            .unwrap_or(12) as usize;
            
        Ok(Qwen2Config {
            vocab_size,
            hidden_size,
            intermediate_size: hidden_size * 4,
            num_hidden_layers,
            num_attention_heads,
            max_position_embeddings: 8192,
            // ... other config parameters
        })
    }
}
```

### 3. Custom Model Wrapper for Nomic Embed Code
```rust
/// Specialized wrapper for Nomic Embed Code model
pub struct NomicEmbedCodeModel {
    model: Qwen2Model,
    device: Device,
    config: Qwen2Config,
}

impl NomicEmbedCodeModel {
    pub fn new(config: Qwen2Config, weights: TensorData, device: Device) -> Result<Self> {
        let model = Qwen2Model::load(&device, config.clone(), weights)?;
        Ok(Self { model, device, config })
    }
    
    /// Generate embeddings from model
    pub fn forward(&mut self, input_ids: &Tensor) -> Result<Tensor> {
        // Get hidden states from the model
        let outputs = self.model.forward(input_ids)?;
        
        // For embedding models, we typically want the last hidden state
        // Apply mean pooling over the sequence dimension
        let embeddings = outputs.mean(1)?;  // Mean over sequence dimension
        
        Ok(embeddings)
    }
}
```

### 4. Update GgufModelWrapper for Nomic Code
```rust
impl GgufModelWrapper {
    /// Create wrapper specifically for nomic-embed-code
    pub async fn new_nomic_embed_code(
        model_path: &Path,
        tokenizer_path: &Path,
        device: Device,
    ) -> Result<Self> {
        // Load tokenizer - research shows Nomic models use custom tokenizers
        let tokenizer = Tokenizer::from_file(tokenizer_path)
            .map_err(|e| ModelLoadingError::TokenizerError { source: e.into() })?;
            
        // Load the model
        let loader = GgufLoader::default();
        let model_info = loader.inspect_gguf_model(model_path)?;
        let model = loader.load_nomic_embed_code(model_path, &device)?;
        
        // Determine actual dimensions from model
        let dimensions = model_info.embedding_length.unwrap_or_else(|| {
            // If not in metadata, we'll need to test with a sample input
            768  // Default assumption, will be updated after testing
        });
        
        Ok(Self {
            model: Box::new(model),
            tokenizer,
            device,
            model_name: "nomic-embed-code.Q5_K_S.gguf".to_string(),
            dimensions,
            context_length: model_info.context_length,
        })
    }
}

#[async_trait]
impl EmbeddingModel for GgufModelWrapper {
    async fn embed(&mut self, texts: &[&str]) -> Result<Vec<Vec<f32>>> {
        let mut embeddings = Vec::new();
        
        for text in texts {
            // Tokenize the input text
            let encoding = self.tokenizer.encode(text, true)
                .map_err(|e| ModelLoadingError::TokenizerError { source: e.into() })?;
                
            let tokens = encoding.get_ids();
            
            // Convert to tensor
            let input_ids = Tensor::new(tokens, &self.device)?
                .unsqueeze(0)?;  // Add batch dimension
                
            // Generate embedding
            let output = self.model.forward(&input_ids)?;
            
            // Convert to Vec<f32>
            let embedding = output.squeeze(0)?.to_vec1::<f32>()?;
            embeddings.push(embedding);
        }
        
        Ok(embeddings)
    }
    
    // ... other trait methods remain the same
}
```

### 5. Find Correct Tokenizer
```rust
impl GgufLoader {
    /// Get the correct tokenizer for nomic-embed-code
    fn get_tokenizer_url(&self, model_info: &ModelInfo) -> Result<String> {
        match model_info.name.as_str() {
            "nomic-embed-code.Q5_K_S.gguf" => {
                // Research indicates nomic models might use a specific tokenizer
                // We may need to extract it from the GGUF file itself or use the original model
                Ok("https://huggingface.co/lmstudio-community/nomic-embed-code-GGUF/resolve/main/tokenizer.json".to_string())
            },
            _ => Err(anyhow::anyhow!("Unknown tokenizer for model {}", model_info.name)),
        }
    }
    
    /// Extract tokenizer from GGUF file if embedded
    fn extract_tokenizer_from_gguf(&self, path: &Path) -> Result<Tokenizer> {
        let gguf = GgufFile::open(path)?;
        
        // Check if tokenizer data is embedded in GGUF
        if let Some(vocab_data) = self.extract_vocab_from_gguf(&gguf) {
            // Build tokenizer from embedded vocab
            self.build_tokenizer_from_vocab(vocab_data)
        } else {
            Err(anyhow::anyhow!("No embedded tokenizer found in GGUF file"))
        }
    }
}
```

### 6. Dimension Detection and Validation
```rust
impl GgufLoader {
    /// Test model to determine actual embedding dimensions
    async fn detect_embedding_dimensions(&self, model: &mut dyn EmbeddingModel) -> Result<usize> {
        // Test with a simple input
        let test_embeddings = model.embed(&["test"]).await?;
        
        if let Some(embedding) = test_embeddings.first() {
            Ok(embedding.len())
        } else {
            Err(anyhow::anyhow!("Failed to generate test embedding"))
        }
    }
}
```

### 7. Update Model Info with Correct Dimensions
```rust
// In models.rs, update the model info once we determine the correct dimensions
ModelInfo::new(
    "nomic-embed-code.Q5_K_S.gguf".to_string(),
    "Code embedding model optimized for programming languages".to_string(),
    // TODO: Update this with actual dimensions after testing
    768,  // Will be updated based on testing
    180_000_000,
    ModelType::Gguf,
    "https://huggingface.co/lmstudio-community/nomic-embed-code-GGUF/resolve/main/nomic-embed-code.Q5_K_S.gguf".to_string(),
    ModelMetadata {
        quantization: Some("Q5_K_S".to_string()),
        architecture: Some("qwen2".to_string()),
        context_length: Some(8192),
    },
)
```

## Test Coverage
Add comprehensive tests:
- Model loading with actual GGUF file
- Embedding generation with code samples
- Dimension validation
- Performance benchmarks with code vs text
- Error handling for corrupted GGUF files

## Acceptance Criteria
- [ ] GGUF file inspection and metadata extraction working
- [ ] Nomic embed code model loading successfully
- [ ] Correct tokenizer identified and working
- [ ] Embedding generation produces reasonable outputs
- [ ] Actual embedding dimensions determined and documented
- [ ] Model works with code samples in multiple languages
- [ ] Performance is acceptable for typical code chunks
- [ ] Comprehensive test coverage
- [ ] Integration with existing model factory works

## Files to Create/Modify
- `src/models.rs` - Complete GGUF loading implementation
- `src/models.rs` - Add NomicEmbedCodeModel implementation
- `src/models.rs` - Update model info with correct dimensions
- `src/models.rs` - Add comprehensive test cases
- `Cargo.toml` - Add gguf-file dependency if needed

## Success Validation
```bash
# Build should succeed
cargo build

# Test with actual model file
cargo test models::test_nomic_embed_code_loading

# Integration test
cargo test models::test_nomic_embed_code_inference

# Dimension validation
cargo test models::test_nomic_embed_code_dimensions
```

## Notes
This step requires actual testing with the GGUF model file to determine correct dimensions and validate the implementation. The model architecture assumptions (Qwen2-based) need to be verified through testing.

If the model is too large for CI/testing, consider using a smaller test model or mocking the heavy parts while keeping the interface tests.