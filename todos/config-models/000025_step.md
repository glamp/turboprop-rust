# Step 25: Add Qwen3-Embedding-0.6B Model Support

## Objective
Complete the implementation of Qwen/Qwen3-Embedding-0.6B model support by implementing the specific Qwen3 architecture, model loading, and 1024-dimensional embedding generation.

## Background
Based on research, Qwen3-Embedding-0.6B produces 1024-dimensional embeddings and supports Matryoshka Representation Learning (MRL). This step implements the specific Qwen3 architecture and completes the Hugging Face model loading infrastructure.

## Tasks
1. Implement Qwen3 model architecture for embedding generation
2. Handle 1024-dimensional output with proper pooling
3. Implement Matryoshka Representation Learning support (optional dimensions)
4. Add comprehensive testing with real model files
5. Optimize performance for the 0.6B parameter model
6. Validate embedding quality and dimensions

## Implementation Details

### 1. Qwen3 Embedding Model Implementation
```rust
use candle_core::{Device, Tensor, DType};
use candle_nn::{Linear, Embedding, LayerNorm};
use safetensors::SafeTensors;

/// Qwen3 embedding model implementation
pub struct Qwen3EmbeddingModel {
    embeddings: Embedding,
    layers: Vec<Qwen3Layer>,
    norm: LayerNorm,
    config: Qwen3Config,
    device: Device,
}

#[derive(Debug, Clone)]
pub struct Qwen3Config {
    pub vocab_size: usize,
    pub hidden_size: usize,
    pub num_hidden_layers: usize,
    pub num_attention_heads: usize,
    pub intermediate_size: usize,
    pub max_position_embeddings: usize,
    pub embedding_dimensions: usize,  // 1024 for Qwen3-Embedding-0.6B
}

impl Qwen3EmbeddingModel {
    /// Create new Qwen3 embedding model
    pub fn new(config: Qwen3Config, weights: &SafeTensors, device: &Device) -> Result<Self> {
        // Load embedding layer
        let embeddings = Self::load_embeddings(&config, weights, device)?;
        
        // Load transformer layers
        let mut layers = Vec::new();
        for i in 0..config.num_hidden_layers {
            let layer = Qwen3Layer::load(&config, weights, i, device)?;
            layers.push(layer);
        }
        
        // Load final layer norm
        let norm = Self::load_layer_norm(&config, weights, "model.norm", device)?;
        
        Ok(Self {
            embeddings,
            layers,
            norm,
            config,
            device: device.clone(),
        })
    }
    
    /// Generate embeddings from input tokens
    pub fn forward(&self, input_ids: &Tensor, attention_mask: &Tensor) -> Result<Tensor> {
        // Embed tokens
        let mut hidden_states = self.embeddings.forward(input_ids)?;
        
        // Apply transformer layers
        for layer in &self.layers {
            hidden_states = layer.forward(&hidden_states, attention_mask)?;
        }
        
        // Apply final layer norm
        hidden_states = self.norm.forward(&hidden_states)?;
        
        Ok(hidden_states)
    }
    
    /// Load embedding layer from weights
    fn load_embeddings(
        config: &Qwen3Config,
        weights: &SafeTensors,
        device: &Device,
    ) -> Result<Embedding> {
        let embeddings_weight = weights.tensor("model.embed_tokens.weight")?;
        let embeddings_tensor = Tensor::from_raw_buffer(
            embeddings_weight.data(),
            embeddings_weight.dtype().try_into()?,
            embeddings_weight.shape(),
            device,
        )?;
        
        Ok(Embedding::new(embeddings_tensor, config.hidden_size))
    }
    
    /// Load layer normalization from weights
    fn load_layer_norm(
        config: &Qwen3Config,
        weights: &SafeTensors,
        prefix: &str,
        device: &Device,
    ) -> Result<LayerNorm> {
        let weight_key = format!("{}.weight", prefix);
        let weight = weights.tensor(&weight_key)?;
        let weight_tensor = Tensor::from_raw_buffer(
            weight.data(),
            weight.dtype().try_into()?,
            weight.shape(),
            device,
        )?;
        
        Ok(LayerNorm::new(weight_tensor, None, 1e-6))
    }
}

/// Individual transformer layer for Qwen3
pub struct Qwen3Layer {
    self_attn: Qwen3Attention,
    mlp: Qwen3MLP,
    input_layernorm: LayerNorm,
    post_attention_layernorm: LayerNorm,
}

impl Qwen3Layer {
    /// Load layer from weights
    pub fn load(
        config: &Qwen3Config,
        weights: &SafeTensors,
        layer_idx: usize,
        device: &Device,
    ) -> Result<Self> {
        let prefix = format!("model.layers.{}", layer_idx);
        
        let self_attn = Qwen3Attention::load(config, weights, &format!("{}.self_attn", prefix), device)?;
        let mlp = Qwen3MLP::load(config, weights, &format!("{}.mlp", prefix), device)?;
        
        let input_layernorm = Qwen3EmbeddingModel::load_layer_norm(
            config,
            weights,
            &format!("{}.input_layernorm", prefix),
            device,
        )?;
        
        let post_attention_layernorm = Qwen3EmbeddingModel::load_layer_norm(
            config,
            weights,
            &format!("{}.post_attention_layernorm", prefix),
            device,
        )?;
        
        Ok(Self {
            self_attn,
            mlp,
            input_layernorm,
            post_attention_layernorm,
        })
    }
    
    /// Forward pass through layer
    pub fn forward(&self, hidden_states: &Tensor, attention_mask: &Tensor) -> Result<Tensor> {
        // Pre-attention layer norm
        let normed = self.input_layernorm.forward(hidden_states)?;
        
        // Self attention
        let attn_output = self.self_attn.forward(&normed, attention_mask)?;
        
        // Residual connection
        let hidden_states = (hidden_states + attn_output)?;
        
        // Pre-MLP layer norm
        let normed = self.post_attention_layernorm.forward(&hidden_states)?;
        
        // MLP
        let mlp_output = self.mlp.forward(&normed)?;
        
        // Residual connection
        let output = (hidden_states + mlp_output)?;
        
        Ok(output)
    }
}

/// Attention mechanism for Qwen3
pub struct Qwen3Attention {
    q_proj: Linear,
    k_proj: Linear,
    v_proj: Linear,
    o_proj: Linear,
    config: Qwen3Config,
}

impl Qwen3Attention {
    pub fn load(
        config: &Qwen3Config,
        weights: &SafeTensors,
        prefix: &str,
        device: &Device,
    ) -> Result<Self> {
        let q_proj = Self::load_linear(weights, &format!("{}.q_proj", prefix), device)?;
        let k_proj = Self::load_linear(weights, &format!("{}.k_proj", prefix), device)?;
        let v_proj = Self::load_linear(weights, &format!("{}.v_proj", prefix), device)?;
        let o_proj = Self::load_linear(weights, &format!("{}.o_proj", prefix), device)?;
        
        Ok(Self {
            q_proj,
            k_proj,
            v_proj,
            o_proj,
            config: config.clone(),
        })
    }
    
    pub fn forward(&self, hidden_states: &Tensor, attention_mask: &Tensor) -> Result<Tensor> {
        let (batch_size, seq_len, hidden_size) = hidden_states.dims3()?;
        
        // Project to Q, K, V
        let q = self.q_proj.forward(hidden_states)?;
        let k = self.k_proj.forward(hidden_states)?;
        let v = self.v_proj.forward(hidden_states)?;
        
        // Reshape for multi-head attention
        let head_dim = hidden_size / self.config.num_attention_heads;
        let q = q.reshape((batch_size, seq_len, self.config.num_attention_heads, head_dim))?
            .transpose(1, 2)?;  // (batch, heads, seq, head_dim)
        let k = k.reshape((batch_size, seq_len, self.config.num_attention_heads, head_dim))?
            .transpose(1, 2)?;
        let v = v.reshape((batch_size, seq_len, self.config.num_attention_heads, head_dim))?
            .transpose(1, 2)?;
        
        // Scaled dot-product attention
        let scale = 1.0 / (head_dim as f64).sqrt();
        let attn_weights = q.matmul(&k.transpose(-2, -1)?)? * scale;
        
        // Apply attention mask
        let attn_weights = self.apply_attention_mask(&attn_weights, attention_mask)?;
        
        // Softmax
        let attn_probs = candle_nn::ops::softmax_last_dim(&attn_weights)?;
        
        // Apply attention to values
        let attn_output = attn_probs.matmul(&v)?;
        
        // Reshape back
        let attn_output = attn_output.transpose(1, 2)?
            .reshape((batch_size, seq_len, hidden_size))?;
        
        // Output projection
        let output = self.o_proj.forward(&attn_output)?;
        
        Ok(output)
    }
    
    fn load_linear(weights: &SafeTensors, key: &str, device: &Device) -> Result<Linear> {
        let weight_key = format!("{}.weight", key);
        let weight = weights.tensor(&weight_key)?;
        let weight_tensor = Tensor::from_raw_buffer(
            weight.data(),
            weight.dtype().try_into()?,
            weight.shape(),
            device,
        )?;
        
        // Check for bias
        let bias_key = format!("{}.bias", key);
        let bias = if weights.tensor(&bias_key).is_ok() {
            let bias = weights.tensor(&bias_key)?;
            Some(Tensor::from_raw_buffer(
                bias.data(),
                bias.dtype().try_into()?,
                bias.shape(),
                device,
            )?)
        } else {
            None
        };
        
        Ok(Linear::new(weight_tensor, bias))
    }
    
    fn apply_attention_mask(&self, attn_weights: &Tensor, mask: &Tensor) -> Result<Tensor> {
        // Convert mask to attention weights shape and apply large negative value to masked positions
        let mask_expanded = mask.unsqueeze(1)?.unsqueeze(1)?; // (batch, 1, 1, seq)
        let mask_value = -1e9_f32;
        
        let masked_weights = attn_weights.where_cond(
            &mask_expanded.eq(&Tensor::ones_like(&mask_expanded)?)?,
            &attn_weights.full_like(mask_value)?,
        )?;
        
        Ok(masked_weights)
    }
}

/// MLP (feed-forward) layer for Qwen3
pub struct Qwen3MLP {
    gate_proj: Linear,
    up_proj: Linear,
    down_proj: Linear,
}

impl Qwen3MLP {
    pub fn load(
        config: &Qwen3Config,
        weights: &SafeTensors,
        prefix: &str,
        device: &Device,
    ) -> Result<Self> {
        let gate_proj = Qwen3Attention::load_linear(weights, &format!("{}.gate_proj", prefix), device)?;
        let up_proj = Qwen3Attention::load_linear(weights, &format!("{}.up_proj", prefix), device)?;
        let down_proj = Qwen3Attention::load_linear(weights, &format!("{}.down_proj", prefix), device)?;
        
        Ok(Self {
            gate_proj,
            up_proj,
            down_proj,
        })
    }
    
    pub fn forward(&self, hidden_states: &Tensor) -> Result<Tensor> {
        let gate = self.gate_proj.forward(hidden_states)?;
        let up = self.up_proj.forward(hidden_states)?;
        
        // SiLU activation: x * sigmoid(x)
        let gate_activated = &gate * &candle_nn::ops::sigmoid(&gate)?;
        
        // Element-wise multiply
        let gated = (&gate_activated * &up)?;
        
        // Down projection
        let output = self.down_proj.forward(&gated)?;
        
        Ok(output)
    }
}
```

### 2. Update HuggingFaceLoader for Qwen3
```rust
impl HuggingFaceLoader {
    /// Load Qwen3 transformer model from weights
    fn load_transformer_model(
        &self,
        weights_path: &Path,
        config: &ModelConfig,
        device: &Device,
    ) -> Result<Qwen3EmbeddingModel> {
        // Load safetensors file
        let weights_data = std::fs::read(weights_path)?;
        let safetensors = SafeTensors::deserialize(&weights_data)
            .map_err(|e| ModelLoadingError::InvalidFormat {
                message: format!("Failed to parse safetensors: {}", e)
            })?;
        
        // Convert to Qwen3Config
        let qwen3_config = Qwen3Config {
            vocab_size: config.vocab_size,
            hidden_size: config.hidden_size,
            num_hidden_layers: config.num_layers,
            num_attention_heads: config.num_attention_heads,
            intermediate_size: config.hidden_size * 4,  // Typical scaling
            max_position_embeddings: config.max_position_embeddings,
            embedding_dimensions: 1024,  // Qwen3-Embedding-0.6B output dimensions
        };
        
        // Load the model
        Qwen3EmbeddingModel::new(qwen3_config, &safetensors, device)
    }
}
```

### 3. Matryoshka Representation Learning Support
```rust
impl HuggingFaceModelWrapper {
    /// Generate embedding with optional dimension truncation (MRL support)
    pub async fn embed_with_dimensions(&mut self, text: &str, dimensions: Option<usize>) -> Result<Vec<f32>> {
        let full_embedding = self.embed_single(text).await?;
        
        if let Some(dims) = dimensions {
            if dims <= full_embedding.len() {
                Ok(full_embedding[..dims].to_vec())
            } else {
                Err(anyhow::anyhow!("Requested dimensions {} exceed model capacity {}", dims, full_embedding.len()))
            }
        } else {
            Ok(full_embedding)
        }
    }
}
```

### 4. Update Model Info with Correct Qwen3 Details
```rust
// In models.rs, update the model info with verified details
ModelInfo::new(
    "Qwen/Qwen3-Embedding-0.6B".to_string(),
    "Qwen3 embedding model with 1024 dimensions and MRL support".to_string(),
    1024,  // Confirmed from research
    600_000_000,  // ~600MB for 0.6B parameters
    ModelType::HuggingFace,
    "Qwen/Qwen3-Embedding-0.6B".to_string(),
    ModelMetadata {
        quantization: None,
        architecture: Some("qwen3".to_string()),
        context_length: Some(32768),  // Confirmed from research
    },
)
```

### 5. Performance Optimizations
```rust
impl Qwen3EmbeddingModel {
    /// Enable half-precision for memory efficiency
    pub fn to_half(&mut self) -> Result<()> {
        // Convert model weights to f16 for memory efficiency
        // Implementation depends on candle's dtype conversion support
        Ok(())
    }
    
    /// Batch processing for multiple texts
    pub fn forward_batch(&self, input_ids: &Tensor, attention_mask: &Tensor) -> Result<Tensor> {
        // Optimized batch processing
        self.forward(input_ids, attention_mask)
    }
}
```

## Test Coverage
Add comprehensive tests:
- Model loading with actual Qwen3 model files
- 1024-dimensional embedding generation
- Matryoshka dimension truncation
- Batch processing performance
- Memory usage validation
- Attention mechanism correctness

## Acceptance Criteria
- [ ] Qwen3EmbeddingModel implements complete transformer architecture
- [ ] Model loading from safetensors works correctly
- [ ] Generates 1024-dimensional embeddings
- [ ] Matryoshka Representation Learning support functional
- [ ] Attention mechanism and MLP layers work correctly
- [ ] Model integrates with HuggingFaceLoader
- [ ] Performance is acceptable for 0.6B parameters
- [ ] Memory usage is reasonable
- [ ] Comprehensive test coverage
- [ ] Integration with model factory works

## Files to Create/Modify
- `src/models.rs` - Add Qwen3EmbeddingModel and related structs
- `src/models.rs` - Complete HuggingFaceLoader transformer loading
- `src/models.rs` - Update model info with correct Qwen3 details
- `src/models.rs` - Add comprehensive test cases
- `Cargo.toml` - Add any missing dependencies

## Success Validation
```bash
# Build should succeed
cargo build

# Test with actual model (may require download)
cargo test models::test_qwen3_loading

# Integration test
cargo test models::test_qwen3_inference

# Dimension validation
cargo test models::test_qwen3_dimensions

# MRL support test
cargo test models::test_qwen3_mrl
```

## Notes
This implementation assumes the standard Qwen3 architecture. The actual model may require adjustments based on the specific variant used for embeddings. The safetensors loading implementation should handle different tensor layouts gracefully.

Consider adding GPU support (CUDA/Metal) for better performance with the 0.6B parameter model. The CPU implementation should work but may be slower for large-scale embedding generation.