# Step 28: Add Comprehensive Testing for Both New Models

## Objective
Implement comprehensive test coverage for both new embedding models, including unit tests, integration tests, performance tests, and end-to-end validation to ensure reliability and correctness.

## Background
With two complex new models (GGUF and Hugging Face transformer), we need thorough testing to validate functionality, performance, and integration. This step creates a comprehensive test suite covering all aspects of the new model implementations.

## Tasks
1. Add unit tests for model loading and inference
2. Create integration tests for end-to-end functionality
3. Add performance benchmarks and memory usage tests
4. Implement model comparison and validation tests
5. Add error handling and edge case testing
6. Create CI/CD friendly test configurations

## Implementation Details

### 1. Model Loading and Inference Unit Tests
```rust
// In src/models.rs, add comprehensive test module

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use std::sync::Once;
    
    static INIT: Once = Once::new();
    
    fn init_test_logging() {
        INIT.call_once(|| {
            tracing_subscriber::fmt::init();
        });
    }

    // Test data for consistent testing
    const TEST_TEXTS: &[&str] = &[
        "function calculateSum(a, b) { return a + b; }",
        "def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)",
        "public class Calculator { public int add(int a, int b) { return a + b; } }",
        "const express = require('express'); const app = express();",
        "import numpy as np; arr = np.array([1, 2, 3, 4, 5])",
    ];

    #[tokio::test]
    async fn test_model_factory_loads_all_types() {
        init_test_logging();
        let factory = ModelFactory::default();
        let temp_dir = TempDir::new().unwrap();
        
        for model_info in ModelManager::get_available_models() {
            // Skip models that require large downloads in CI
            if should_skip_model_in_ci(&model_info.name) {
                continue;
            }
            
            let result = factory.load_model(&model_info, temp_dir.path()).await;
            match result {
                Ok(_) => println!("✓ Successfully loaded {}", model_info.name),
                Err(e) => {
                    // Some models may fail due to network/download issues in CI
                    if is_acceptable_ci_failure(&e) {
                        println!("⚠ Acceptable CI failure for {}: {}", model_info.name, e);
                    } else {
                        panic!("Failed to load {}: {}", model_info.name, e);
                    }
                }
            }
        }
    }

    #[tokio::test]
    async fn test_fastembed_model_wrapper() {
        init_test_logging();
        let temp_dir = TempDir::new().unwrap();
        let model_info = ModelManager::get_available_models()
            .into_iter()
            .find(|m| m.model_type == ModelType::FastEmbed)
            .unwrap();
            
        let loader = FastEmbedLoader;
        let mut model = loader.load_model(&model_info, temp_dir.path()).await.unwrap();
        
        // Test single embedding
        let embeddings = model.embed(&["test"]).await.unwrap();
        assert_eq!(embeddings.len(), 1);
        assert_eq!(embeddings[0].len(), model_info.dimensions);
        
        // Test batch embedding
        let embeddings = model.embed(TEST_TEXTS).await.unwrap();
        assert_eq!(embeddings.len(), TEST_TEXTS.len());
        for embedding in &embeddings {
            assert_eq!(embedding.len(), model_info.dimensions);
            assert!(!embedding.iter().all(|&x| x == 0.0)); // Not all zeros
        }
    }

    #[tokio::test]
    #[ignore = "requires model download"] // Use with --ignored flag
    async fn test_gguf_model_wrapper() {
        init_test_logging();
        let temp_dir = TempDir::new().unwrap();
        let model_info = ModelManager::get_available_models()
            .into_iter()
            .find(|m| m.name == "nomic-embed-code.Q5_K_S.gguf")
            .unwrap();
            
        let loader = GgufLoader::default();
        let mut model = loader.load_model(&model_info, temp_dir.path()).await.unwrap();
        
        // Test with code samples
        let code_samples = &[
            "fn main() { println!(\"Hello, world!\"); }",
            "class Example { public void method() {} }",
            "def function(): pass",
        ];
        
        let embeddings = model.embed(code_samples).await.unwrap();
        assert_eq!(embeddings.len(), code_samples.len());
        
        for embedding in &embeddings {
            assert_eq!(embedding.len(), model_info.dimensions);
            // Validate embedding quality (not all zeros, reasonable variance)
            let mean = embedding.iter().sum::<f32>() / embedding.len() as f32;
            let variance = embedding.iter()
                .map(|&x| (x - mean).powi(2))
                .sum::<f32>() / embedding.len() as f32;
            assert!(variance > 1e-6, "Embedding has insufficient variance");
        }
    }

    #[tokio::test]
    #[ignore = "requires model download"] // Use with --ignored flag
    async fn test_huggingface_model_wrapper() {
        init_test_logging();
        let temp_dir = TempDir::new().unwrap();
        let model_info = ModelManager::get_available_models()
            .into_iter()
            .find(|m| m.name == "Qwen/Qwen3-Embedding-0.6B")
            .unwrap();
            
        let loader = HuggingFaceLoader::default();
        let mut model = loader.load_model(&model_info, temp_dir.path()).await.unwrap();
        
        // Test dimensions
        assert_eq!(model.embedding_dimensions(), 1024);
        
        // Test embedding generation
        let embeddings = model.embed(TEST_TEXTS).await.unwrap();
        assert_eq!(embeddings.len(), TEST_TEXTS.len());
        
        for embedding in &embeddings {
            assert_eq!(embedding.len(), 1024);
            
            // Test that embeddings are normalized (common for modern models)
            let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
            assert!((0.8..=1.2).contains(&norm), "Embedding norm should be close to 1.0, got {}", norm);
        }
    }

    // Helper functions for CI testing
    fn should_skip_model_in_ci(model_name: &str) -> bool {
        if std::env::var("CI").is_ok() || std::env::var("GITHUB_ACTIONS").is_ok() {
            // Skip large models in CI
            matches!(model_name, 
                "nomic-embed-code.Q5_K_S.gguf" | 
                "Qwen/Qwen3-Embedding-0.6B"
            )
        } else {
            false
        }
    }
    
    fn is_acceptable_ci_failure(error: &anyhow::Error) -> bool {
        let error_str = error.to_string().to_lowercase();
        error_str.contains("download") || 
        error_str.contains("network") || 
        error_str.contains("timeout") ||
        error_str.contains("connection")
    }
}
```

### 2. Integration Tests
```rust
// Create tests/model_integration_tests.rs

use anyhow::Result;
use tempfile::TempDir;
use turboprop::embeddings::{EmbeddingGenerator, EmbeddingConfig};
use turboprop::models::ModelManager;

#[tokio::test]
async fn test_embedding_generator_with_all_models() -> Result<()> {
    let temp_dir = TempDir::new()?;
    
    for model_info in ModelManager::get_available_models() {
        if should_skip_large_model(&model_info.name) {
            continue;
        }
        
        println!("Testing EmbeddingGenerator with {}", model_info.name);
        
        let config = EmbeddingConfig::with_model(&model_info.name)
            .with_cache_dir(temp_dir.path());
            
        let mut generator = EmbeddingGenerator::new(config).await?;
        
        // Test basic functionality
        let embedding = generator.embed_single("test function").await?;
        assert_eq!(embedding.len(), model_info.dimensions);
        
        // Test batch processing
        let texts = vec![
            "function add(a, b) { return a + b; }".to_string(),
            "def multiply(x, y): return x * y".to_string(),
        ];
        let embeddings = generator.embed_batch(&texts).await?;
        assert_eq!(embeddings.len(), 2);
        
        println!("✓ {} working correctly", model_info.name);
    }
    
    Ok(())
}

#[tokio::test]
async fn test_model_consistency() -> Result<()> {
    // Test that same input produces same output
    let test_text = "consistent test input";
    let temp_dir = TempDir::new()?;
    
    for model_info in ModelManager::get_available_models() {
        if should_skip_large_model(&model_info.name) {
            continue;
        }
        
        let config = EmbeddingConfig::with_model(&model_info.name)
            .with_cache_dir(temp_dir.path());
            
        let mut generator1 = EmbeddingGenerator::new(config.clone()).await?;
        let mut generator2 = EmbeddingGenerator::new(config).await?;
        
        let embedding1 = generator1.embed_single(test_text).await?;
        let embedding2 = generator2.embed_single(test_text).await?;
        
        // Should be identical
        assert_eq!(embedding1.len(), embedding2.len());
        for (a, b) in embedding1.iter().zip(embedding2.iter()) {
            assert!((a - b).abs() < 1e-6, "Embeddings should be consistent");
        }
    }
    
    Ok(())
}

#[tokio::test]
async fn test_model_performance_comparison() -> Result<()> {
    let test_texts: Vec<String> = (0..10)
        .map(|i| format!("function test{}() {{ return {}; }}", i, i))
        .collect();
    
    let temp_dir = TempDir::new()?;
    let mut results = Vec::new();
    
    for model_info in ModelManager::get_available_models() {
        if should_skip_large_model(&model_info.name) {
            continue;
        }
        
        let config = EmbeddingConfig::with_model(&model_info.name)
            .with_cache_dir(temp_dir.path());
            
        let mut generator = EmbeddingGenerator::new(config).await?;
        
        let start = std::time::Instant::now();
        let embeddings = generator.embed_batch(&test_texts).await?;
        let duration = start.elapsed();
        
        let texts_per_second = test_texts.len() as f64 / duration.as_secs_f64();
        
        results.push((
            model_info.name.clone(),
            duration,
            texts_per_second,
            embeddings[0].len(),
        ));
        
        println!(
            "{}: {:.2}ms total, {:.1} texts/sec, {} dims",
            model_info.name,
            duration.as_millis(),
            texts_per_second,
            embeddings[0].len()
        );
    }
    
    // Print performance summary
    println!("\nPerformance Summary:");
    results.sort_by(|a, b| b.2.partial_cmp(&a.2).unwrap());
    for (name, duration, tps, dims) in results {
        println!(
            "{:35} {:8.1} texts/sec {:8}ms {:4} dims",
            name,
            tps,
            duration.as_millis(),
            dims
        );
    }
    
    Ok(())
}

fn should_skip_large_model(model_name: &str) -> bool {
    if std::env::var("SKIP_LARGE_MODELS").is_ok() {
        return true;
    }
    
    // Skip in CI environments
    if std::env::var("CI").is_ok() {
        matches!(model_name, 
            "nomic-embed-code.Q5_K_S.gguf" | 
            "Qwen/Qwen3-Embedding-0.6B"
        )
    } else {
        false
    }
}
```

### 3. Performance Benchmarks
```rust
// Create benches/model_benchmarks.rs

use criterion::{criterion_group, criterion_main, BenchmarkId, Criterion, Throughput};
use turboprop::embeddings::{EmbeddingGenerator, EmbeddingConfig};
use turboprop::models::ModelManager;
use tokio::runtime::Runtime;

fn benchmark_embedding_generation(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    
    let test_texts: Vec<String> = vec![
        "function calculateSum(a, b) { return a + b; }".to_string(),
        "def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)".to_string(),
        "public class Calculator { public int add(int a, int b) { return a + b; } }".to_string(),
    ];
    
    let temp_dir = tempfile::TempDir::new().unwrap();
    
    // Benchmark each model
    for model_info in ModelManager::get_available_models() {
        if should_skip_model_for_bench(&model_info.name) {
            continue;
        }
        
        let config = EmbeddingConfig::with_model(&model_info.name)
            .with_cache_dir(temp_dir.path());
            
        let generator = rt.block_on(async {
            EmbeddingGenerator::new(config).await.unwrap()
        });
        
        let mut group = c.benchmark_group("embedding_generation");
        group.throughput(Throughput::Elements(test_texts.len() as u64));
        
        group.bench_with_input(
            BenchmarkId::new("model", &model_info.name),
            &test_texts,
            |b, texts| {
                b.to_async(&rt).iter(|| async {
                    let mut gen = generator.clone(); // This won't work, need to restructure
                    gen.embed_batch(texts).await.unwrap()
                });
            },
        );
        
        group.finish();
    }
}

fn benchmark_single_embedding(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let test_text = "function example() { return 42; }";
    let temp_dir = tempfile::TempDir::new().unwrap();
    
    let mut group = c.benchmark_group("single_embedding");
    
    for model_info in ModelManager::get_available_models() {
        if should_skip_model_for_bench(&model_info.name) {
            continue;
        }
        
        group.bench_function(
            BenchmarkId::new("model", &model_info.name),
            |b| {
                b.to_async(&rt).iter(|| async {
                    let config = EmbeddingConfig::with_model(&model_info.name)
                        .with_cache_dir(temp_dir.path());
                    let mut generator = EmbeddingGenerator::new(config).await.unwrap();
                    generator.embed_single(test_text).await.unwrap()
                });
            },
        );
    }
    
    group.finish();
}

fn should_skip_model_for_bench(model_name: &str) -> bool {
    // Skip large models in benchmarks unless specifically requested
    if std::env::var("BENCH_ALL_MODELS").is_err() {
        matches!(model_name, 
            "nomic-embed-code.Q5_K_S.gguf" | 
            "Qwen/Qwen3-Embedding-0.6B"
        )
    } else {
        false
    }
}

criterion_group!(benches, benchmark_embedding_generation, benchmark_single_embedding);
criterion_main!(benches);
```

### 4. Error Handling and Edge Case Tests
```rust
// Add to tests/error_handling_tests.rs

#[tokio::test]
async fn test_invalid_model_names() {
    let invalid_models = vec![
        "nonexistent-model",
        "invalid/model/name",
        "",
        "model with spaces",
    ];
    
    for invalid_model in invalid_models {
        let result = EmbeddingGenerator::from_model_name(invalid_model).await;
        assert!(result.is_err(), "Should fail for invalid model: {}", invalid_model);
        
        let error_msg = result.unwrap_err().to_string();
        assert!(error_msg.contains("Unknown model") || error_msg.contains("not found"));
    }
}

#[tokio::test]
async fn test_large_text_handling() {
    let config = EmbeddingConfig::with_model("sentence-transformers/all-MiniLM-L6-v2");
    let mut generator = EmbeddingGenerator::new(config).await.unwrap();
    
    // Test with very large text (should be truncated or handled gracefully)
    let large_text = "x".repeat(100_000);
    let result = generator.embed_single(&large_text).await;
    
    match result {
        Ok(embedding) => {
            assert_eq!(embedding.len(), 384); // Default model dimensions
        }
        Err(e) => {
            // Acceptable to fail with large text, but error should be informative
            let error_msg = e.to_string();
            assert!(
                error_msg.contains("too long") || 
                error_msg.contains("context") || 
                error_msg.contains("truncated")
            );
        }
    }
}

#[tokio::test]
async fn test_empty_and_special_inputs() {
    let config = EmbeddingConfig::with_model("sentence-transformers/all-MiniLM-L6-v2");
    let mut generator = EmbeddingGenerator::new(config).await.unwrap();
    
    let special_inputs = vec![
        "",
        " ",
        "\n\t\r",
        "🚀🔥💻",
        "كود جافاسكريبت",
        "コードサンプル",
    ];
    
    for input in special_inputs {
        let result = generator.embed_single(input).await;
        assert!(result.is_ok(), "Should handle special input: '{}'", input);
        
        let embedding = result.unwrap();
        assert_eq!(embedding.len(), 384);
        
        // Should not be all zeros (except for empty string)
        if !input.trim().is_empty() {
            assert!(embedding.iter().any(|&x| x != 0.0));
        }
    }
}
```

### 5. CI/CD Configuration
```yaml
# Add to .github/workflows/test.yml

name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
    
    - name: Run unit tests
      run: cargo test --lib
    
    - name: Run integration tests (fast models only)
      run: cargo test --test '*' 
      env:
        SKIP_LARGE_MODELS: "1"
        CI: "1"
    
    - name: Run benchmarks (fast models only)
      run: cargo bench
      env:
        BENCH_FAST_ONLY: "1"
  
  test-full:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v3
    - uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
    
    - name: Run all tests including large models
      run: cargo test --test '*' -- --ignored
      env:
        RUST_LOG: "info"
```

## Test Coverage Goals
- [ ] Unit tests for all model loaders and wrappers
- [ ] Integration tests for EmbeddingGenerator with all models
- [ ] Performance benchmarks for all models
- [ ] Error handling for invalid inputs and edge cases
- [ ] Memory usage validation for large models
- [ ] Consistency tests (same input → same output)
- [ ] CI/CD integration with appropriate model skipping

## Acceptance Criteria
- [ ] All unit tests pass with fast models
- [ ] Integration tests work with model downloads
- [ ] Performance benchmarks provide useful metrics
- [ ] Error handling covers all edge cases
- [ ] CI runs efficiently (skips large downloads)
- [ ] Test coverage is >90% for new model code
- [ ] Documentation includes testing instructions
- [ ] Memory usage stays within reasonable bounds

## Files to Create/Modify
- `src/models.rs` - Add comprehensive unit tests
- `tests/model_integration_tests.rs` - New integration test file
- `tests/error_handling_tests.rs` - New error handling tests
- `benches/model_benchmarks.rs` - New benchmark file
- `.github/workflows/test.yml` - Update CI configuration
- `Cargo.toml` - Add dev dependencies for testing

## Success Validation
```bash
# Run all fast tests
cargo test

# Run integration tests with large models (local only)
cargo test --test '*' -- --ignored

# Run benchmarks
cargo bench

# Test with specific models
TURBOPROP_MODEL=nomic-embed-code.Q5_K_S.gguf cargo test --test model_integration_tests

# Memory usage test
RUST_LOG=debug cargo test test_memory_usage
```

## Notes
This comprehensive test suite ensures reliability while being CI-friendly. Large model tests are marked with `#[ignore]` and can be run locally or in special CI conditions. The benchmarks help track performance regressions and compare models objectively.