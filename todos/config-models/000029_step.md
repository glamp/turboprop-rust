# Step 29: Performance Optimization and Memory Management

## Objective
Optimize performance and memory usage for both new embedding models, implementing caching strategies, memory-efficient batch processing, and model-specific optimizations to ensure good performance even with large models.

## Background
The two new models have different performance characteristics: nomic-embed-code.Q5_K_S.gguf is a large quantized model, while Qwen/Qwen3-Embedding-0.6B is a full-precision transformer. This step implements optimizations specific to each model type to maximize performance while maintaining reasonable memory usage.

## Tasks
1. Implement model-specific memory optimizations
2. Add intelligent caching and model reuse strategies
3. Optimize batch processing for each model type
4. Implement lazy loading and model warm-up
5. Add memory monitoring and cleanup
6. Create performance profiling and debugging tools

## Implementation Details

### 1. Model-Specific Memory Optimizations
```rust
// In src/models.rs, add memory optimization traits

/// Trait for models that support memory optimization
pub trait MemoryOptimizable {
    /// Reduce memory footprint (may impact performance)
    fn optimize_memory(&mut self) -> Result<()>;
    
    /// Estimate current memory usage in bytes
    fn memory_usage(&self) -> usize;
    
    /// Clear internal caches to free memory
    fn clear_caches(&mut self);
    
    /// Check if model supports half-precision
    fn supports_half_precision(&self) -> bool;
    
    /// Convert to half precision if supported
    fn to_half_precision(&mut self) -> Result<()>;
}

// Implement for GGUF models
impl MemoryOptimizable for GgufModelWrapper {
    fn optimize_memory(&mut self) -> Result<()> {
        // GGUF models are already quantized, but we can optimize batch sizes
        self.clear_caches();
        Ok(())
    }
    
    fn memory_usage(&self) -> usize {
        // Estimate based on model size and current state
        // GGUF models are typically more memory efficient
        match self.model_name.as_str() {
            "nomic-embed-code.Q5_K_S.gguf" => 200_000_000, // ~200MB estimated
            _ => 100_000_000,
        }
    }
    
    fn clear_caches(&mut self) {
        // Clear any internal caches or temporary buffers
        // Implementation depends on the specific GGUF loader
    }
    
    fn supports_half_precision(&self) -> bool {
        false // Already quantized
    }
    
    fn to_half_precision(&mut self) -> Result<()> {
        // No-op for quantized models
        Ok(())
    }
}

// Implement for Hugging Face models
impl MemoryOptimizable for HuggingFaceModelWrapper {
    fn optimize_memory(&mut self) -> Result<()> {
        if self.supports_half_precision() {
            self.to_half_precision()?;
        }
        self.clear_caches();
        Ok(())
    }
    
    fn memory_usage(&self) -> usize {
        match self.model_name.as_str() {
            "Qwen/Qwen3-Embedding-0.6B" => {
                // 0.6B parameters * 4 bytes (f32) = ~2.4GB
                // With optimizations, can be reduced
                if self.is_half_precision() {
                    1_200_000_000 // ~1.2GB in half precision
                } else {
                    2_400_000_000 // ~2.4GB in full precision
                }
            }
            _ => 500_000_000,
        }
    }
    
    fn clear_caches(&mut self) {
        // Clear attention caches, KV caches, etc.
        // Implementation depends on the specific model architecture
    }
    
    fn supports_half_precision(&self) -> bool {
        // Most modern models support f16
        true
    }
    
    fn to_half_precision(&mut self) -> Result<()> {
        // Convert model weights to f16
        // This is model-specific and depends on candle's dtype support
        if let Some(qwen_model) = self.model.as_qwen3_mut() {
            qwen_model.to_half()?;
        }
        Ok(())
    }
}
```

### 2. Intelligent Model Caching and Reuse
```rust
// Create src/model_cache.rs

use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use tokio::sync::Mutex;
use std::time::{Duration, Instant};

/// Cache for loaded models to avoid reloading
pub struct ModelCache {
    models: Arc<RwLock<HashMap<String, CachedModel>>>,
    max_cache_size: usize,
    cleanup_interval: Duration,
}

struct CachedModel {
    model: Arc<Mutex<Box<dyn EmbeddingModel>>>,
    last_used: Instant,
    memory_usage: usize,
}

impl ModelCache {
    pub fn new(max_cache_size: usize) -> Self {
        let cache = Self {
            models: Arc::new(RwLock::new(HashMap::new())),
            max_cache_size,
            cleanup_interval: Duration::from_secs(300), // 5 minutes
        };
        
        // Start cleanup task
        cache.start_cleanup_task();
        cache
    }
    
    /// Get or load a model from cache
    pub async fn get_or_load_model(
        &self,
        model_name: String,
        factory: &ModelFactory,
        cache_dir: &Path,
    ) -> Result<Arc<Mutex<Box<dyn EmbeddingModel>>>> {
        // Check cache first
        {
            let models = self.models.read().unwrap();
            if let Some(cached) = models.get(&model_name) {
                // Update last used time
                drop(models);
                let mut models = self.models.write().unwrap();
                if let Some(cached) = models.get_mut(&model_name) {
                    cached.last_used = Instant::now();
                }
                return Ok(cached.model.clone());
            }
        }
        
        // Load model
        let model_info = ModelManager::get_model_info(&model_name)
            .ok_or_else(|| anyhow::anyhow!("Unknown model: {}", model_name))?;
            
        let mut model = factory.load_model(&model_info, cache_dir).await?;
        
        // Optimize memory if possible
        if let Ok(optimizable) = model.as_any_mut().downcast_mut::<dyn MemoryOptimizable>() {
            optimizable.optimize_memory()?;
        }
        
        let memory_usage = self.estimate_model_memory(&model);
        let model = Arc::new(Mutex::new(model));
        
        // Add to cache
        let cached = CachedModel {
            model: model.clone(),
            last_used: Instant::now(),
            memory_usage,
        };
        
        let mut models = self.models.write().unwrap();
        models.insert(model_name, cached);
        
        // Check if we need to evict old models
        self.maybe_evict_models(&mut models)?;
        
        Ok(model)
    }
    
    /// Start background cleanup task
    fn start_cleanup_task(&self) {
        let models = self.models.clone();
        let cleanup_interval = self.cleanup_interval;
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(cleanup_interval);
            
            loop {
                interval.tick().await;
                
                let mut models_guard = models.write().unwrap();
                let cutoff = Instant::now() - cleanup_interval * 2;
                
                // Remove models not used recently
                models_guard.retain(|name, cached| {
                    if cached.last_used < cutoff {
                        info!("Evicting unused model from cache: {}", name);
                        false
                    } else {
                        true
                    }
                });
            }
        });
    }
    
    /// Evict models if cache is too large
    fn maybe_evict_models(&self, models: &mut HashMap<String, CachedModel>) -> Result<()> {
        let total_memory: usize = models.values().map(|m| m.memory_usage).sum();
        
        if total_memory > self.max_cache_size {
            // Sort by last used time, evict oldest
            let mut entries: Vec<_> = models.iter().map(|(name, cached)| {
                (name.clone(), cached.last_used, cached.memory_usage)
            }).collect();
            
            entries.sort_by_key(|(_, last_used, _)| *last_used);
            
            let mut freed_memory = 0;
            for (name, _, memory) in entries {
                if total_memory - freed_memory <= self.max_cache_size {
                    break;
                }
                
                info!("Evicting model from cache due to memory pressure: {}", name);
                models.remove(&name);
                freed_memory += memory;
            }
        }
        
        Ok(())
    }
    
    fn estimate_model_memory(&self, model: &Box<dyn EmbeddingModel>) -> usize {
        // Try to get accurate memory usage if model supports it
        if let Ok(optimizable) = model.as_any().downcast_ref::<dyn MemoryOptimizable>() {
            optimizable.memory_usage()
        } else {
            // Fallback estimation based on model type
            match model.model_type() {
                ModelType::FastEmbed => 50_000_000,    // ~50MB
                ModelType::Gguf => 200_000_000,        // ~200MB
                ModelType::HuggingFace => 1_000_000_000, // ~1GB
            }
        }
    }
}
```

### 3. Optimized Batch Processing
```rust
// In src/embeddings.rs, add advanced batch processing

impl EmbeddingGenerator {
    /// Advanced batch processing with memory management
    pub async fn embed_batch_advanced(
        &mut self,
        texts: &[String],
        options: BatchProcessingOptions,
    ) -> Result<Vec<Vec<f32>>> {
        if texts.is_empty() {
            return Ok(Vec::new());
        }
        
        let optimal_batch_size = self.calculate_advanced_batch_size(texts, &options).await;
        let mut all_embeddings = Vec::with_capacity(texts.len());
        
        // Process with memory monitoring
        let memory_monitor = MemoryMonitor::new(options.max_memory_usage);
        
        for (batch_idx, chunk) in texts.chunks(optimal_batch_size).enumerate() {
            // Check memory before processing
            memory_monitor.check_memory_usage()?;
            
            // Process batch with progress tracking
            let batch_start = std::time::Instant::now();
            let text_refs: Vec<&str> = chunk.iter().map(|s| s.as_str()).collect();
            
            let batch_embeddings = self.model.embed(&text_refs).await
                .with_context(|| format!("Failed batch {} of {}", batch_idx + 1, texts.len() / optimal_batch_size + 1))?;
            
            all_embeddings.extend(batch_embeddings);
            
            // Log progress and performance
            let batch_duration = batch_start.elapsed();
            let texts_per_second = chunk.len() as f64 / batch_duration.as_secs_f64();
            
            if batch_idx % 10 == 0 || batch_duration > Duration::from_secs(5) {
                info!(
                    "Batch {}/{}: {:.1} texts/sec, {:.2}ms, memory: {}MB",
                    batch_idx + 1,
                    (texts.len() + optimal_batch_size - 1) / optimal_batch_size,
                    texts_per_second,
                    batch_duration.as_millis(),
                    memory_monitor.current_usage_mb()
                );
            }
            
            // Optional yield for cooperative scheduling
            if options.yield_between_batches {
                tokio::task::yield_now().await;
            }
        }
        
        Ok(all_embeddings)
    }
    
    async fn calculate_advanced_batch_size(
        &self,
        texts: &[String],
        options: &BatchProcessingOptions,
    ) -> usize {
        let base_size = self.calculate_optimal_batch_size(texts).await;
        
        // Adjust based on available memory
        let available_memory = self.get_available_memory();
        let model_memory = self.estimate_model_memory();
        let safe_memory = available_memory.saturating_sub(model_memory) / 2;
        
        // Estimate memory per text
        let avg_text_length = texts.iter().map(|t| t.len()).sum::<usize>() / texts.len().max(1);
        let memory_per_text = self.estimate_memory_per_text(avg_text_length);
        
        let memory_limited_size = if memory_per_text > 0 {
            (safe_memory / memory_per_text).max(1)
        } else {
            base_size
        };
        
        // Take minimum of all constraints
        base_size
            .min(memory_limited_size)
            .min(options.max_batch_size.unwrap_or(usize::MAX))
            .max(options.min_batch_size.unwrap_or(1))
    }
    
    fn estimate_memory_per_text(&self, avg_text_length: usize) -> usize {
        match self.model.model_type() {
            ModelType::FastEmbed => avg_text_length * 4, // Rough estimate
            ModelType::Gguf => avg_text_length * 2,      // More efficient
            ModelType::HuggingFace => avg_text_length * 8, // Higher memory usage
        }
    }
    
    fn get_available_memory(&self) -> usize {
        sys_info::mem_info()
            .map(|info| (info.avail * 1024) as usize) // Convert KB to bytes
            .unwrap_or(1_000_000_000) // 1GB fallback
    }
    
    fn estimate_model_memory(&self) -> usize {
        // Try to get from model if it implements MemoryOptimizable
        if let Ok(optimizable) = self.model.as_any().downcast_ref::<dyn MemoryOptimizable>() {
            optimizable.memory_usage()
        } else {
            // Fallback estimates
            match self.model.model_type() {
                ModelType::FastEmbed => 100_000_000,
                ModelType::Gguf => 300_000_000,
                ModelType::HuggingFace => 1_500_000_000,
            }
        }
    }
}

#[derive(Debug, Clone)]
pub struct BatchProcessingOptions {
    pub max_batch_size: Option<usize>,
    pub min_batch_size: Option<usize>,
    pub max_memory_usage: usize, // in bytes
    pub yield_between_batches: bool,
    pub enable_progress_reporting: bool,
}

impl Default for BatchProcessingOptions {
    fn default() -> Self {
        Self {
            max_batch_size: None,
            min_batch_size: Some(1),
            max_memory_usage: 2_000_000_000, // 2GB
            yield_between_batches: true,
            enable_progress_reporting: true,
        }
    }
}
```

### 4. Memory Monitor
```rust
// Create src/memory_monitor.rs

use std::time::{Duration, Instant};
use tracing::{warn, debug};

pub struct MemoryMonitor {
    max_memory: usize,
    last_check: Instant,
    check_interval: Duration,
}

impl MemoryMonitor {
    pub fn new(max_memory: usize) -> Self {
        Self {
            max_memory,
            last_check: Instant::now(),
            check_interval: Duration::from_secs(1),
        }
    }
    
    pub fn check_memory_usage(&mut self) -> Result<()> {
        let now = Instant::now();
        if now.duration_since(self.last_check) < self.check_interval {
            return Ok(());
        }
        
        self.last_check = now;
        
        if let Ok(mem_info) = sys_info::mem_info() {
            let used_memory = (mem_info.total - mem_info.avail) * 1024; // Convert to bytes
            
            if used_memory as usize > self.max_memory {
                warn!(
                    "Memory usage ({:.1}GB) exceeds limit ({:.1}GB)",
                    used_memory as f64 / 1e9,
                    self.max_memory as f64 / 1e9
                );
                
                // Force garbage collection if possible
                self.force_cleanup();
                
                return Err(anyhow::anyhow!(
                    "Memory usage limit exceeded: {:.1}GB > {:.1}GB",
                    used_memory as f64 / 1e9,
                    self.max_memory as f64 / 1e9
                ));
            }
            
            debug!(
                "Memory usage: {:.1}GB / {:.1}GB",
                used_memory as f64 / 1e9,
                self.max_memory as f64 / 1e9
            );
        }
        
        Ok(())
    }
    
    pub fn current_usage_mb(&self) -> u64 {
        sys_info::mem_info()
            .map(|info| (info.total - info.avail) / 1024) // Convert to MB
            .unwrap_or(0)
    }
    
    fn force_cleanup(&self) {
        // Force garbage collection if available
        // This is limited in Rust, but we can try to free caches
        
        // Clear any global caches
        // Implementation depends on specific caching systems in use
    }
}
```

### 5. Performance Profiling Tools
```rust
// Add to src/profiling.rs

use std::collections::HashMap;
use std::time::{Duration, Instant};

pub struct PerformanceProfiler {
    measurements: HashMap<String, Vec<Duration>>,
    current_timers: HashMap<String, Instant>,
}

impl PerformanceProfiler {
    pub fn new() -> Self {
        Self {
            measurements: HashMap::new(),
            current_timers: HashMap::new(),
        }
    }
    
    pub fn start_timer(&mut self, name: &str) {
        self.current_timers.insert(name.to_string(), Instant::now());
    }
    
    pub fn end_timer(&mut self, name: &str) {
        if let Some(start_time) = self.current_timers.remove(name) {
            let duration = start_time.elapsed();
            self.measurements
                .entry(name.to_string())
                .or_insert_with(Vec::new)
                .push(duration);
        }
    }
    
    pub fn print_summary(&self) {
        println!("Performance Summary:");
        println!("==================");
        
        for (name, durations) in &self.measurements {
            if durations.is_empty() {
                continue;
            }
            
            let total: Duration = durations.iter().sum();
            let avg = total / durations.len() as u32;
            let min = durations.iter().min().unwrap();
            let max = durations.iter().max().unwrap();
            
            println!(
                "{:30} Count: {:4} Avg: {:6.2}ms Min: {:6.2}ms Max: {:6.2}ms Total: {:8.2}ms",
                name,
                durations.len(),
                avg.as_secs_f64() * 1000.0,
                min.as_secs_f64() * 1000.0,
                max.as_secs_f64() * 1000.0,
                total.as_secs_f64() * 1000.0,
            );
        }
    }
}

// Convenience macro for profiling
#[macro_export]
macro_rules! profile {
    ($profiler:expr, $name:expr, $code:block) => {
        $profiler.start_timer($name);
        let result = $code;
        $profiler.end_timer($name);
        result
    };
}
```

## Acceptance Criteria
- [ ] Model-specific memory optimizations implemented
- [ ] Intelligent model caching reduces reload times
- [ ] Batch processing is memory-aware and efficient
- [ ] Memory monitoring prevents out-of-memory errors
- [ ] Performance profiling tools provide useful metrics
- [ ] Memory usage is reasonable for all model types
- [ ] Performance improvements are measurable
- [ ] Large models can run on systems with limited memory
- [ ] Comprehensive testing validates optimizations

## Files to Create/Modify
- `src/models.rs` - Add MemoryOptimizable trait and implementations
- `src/model_cache.rs` - New model caching system
- `src/embeddings.rs` - Advanced batch processing
- `src/memory_monitor.rs` - Memory monitoring utilities
- `src/profiling.rs` - Performance profiling tools
- `src/lib.rs` - Export new modules

## Success Validation
```bash
# Test memory optimizations
RUST_LOG=debug cargo test test_memory_optimization

# Benchmark with large batches
cargo bench --bench model_benchmarks

# Test with memory constraints
TURBOPROP_MAX_MEMORY=1GB cargo test test_large_batch_processing

# Profile memory usage
cargo run --release --example profile_memory
```

## Notes
These optimizations are especially important for the large models. The caching system prevents repeated model loading, while the memory monitor ensures the system doesn't run out of memory during large batch processing operations.

The performance improvements should be measurable through the existing benchmark suite and validated in real-world usage scenarios.